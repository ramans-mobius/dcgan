name: 2 DCGAN Image Preprocessor
description: Preprocess images for DCGAN training - resize, normalize, and create train/test splits.
inputs:
  - {name: original_dataset, type: Dataset, description: "Original DataWrapper pickle with images"}
  - {name: train_split_info, type: Data, description: "JSON with train/test split indices"}
  - {name: image_size, type: Integer, optional: true, default: "64", description: "Target image size"}
  - {name: normalize, type: Boolean, optional: true, default: "true", description: "Normalize images to [-1, 1]"}
outputs:
  - {name: train_images, type: Dataset, description: "Pickled train images tensor"}
  - {name: test_images, type: Dataset, description: "Pickled test images tensor"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v32
    command:
      - sh
      - -c
      - |
        python3 -c "
        import sys, json, os, pickle, torch, numpy as np
        from datetime import datetime
        import io
        from PIL import Image
        from torchvision import transforms

        print('Number of arguments:', len(sys.argv))
        for i, arg in enumerate(sys.argv):
            print(f'  Argument {i}: {arg[:100] if len(str(arg)) > 100 else arg}')
        
        # Get args - match working pattern
        if len(sys.argv) < 7:
            raise ValueError(f'Expected 7 arguments, got {len(sys.argv)}')
        
        original_dataset_path = sys.argv[1]      # $0
        train_split_info_path = sys.argv[2]      # $1
        image_size = int(sys.argv[3])            # $2
        normalize_str = sys.argv[4]              # $3
        train_images_path = sys.argv[5]          # $4
        test_images_path = sys.argv[6]           # $5

        normalize = normalize_str.lower() == 'true'

        print('='*80)
        print('DCGAN IMAGE PREPROCESSOR')
        print('='*80)
        print(f'Image size: {image_size}x{image_size}')
        print(f'Normalize: {normalize}')

        # Load original dataset
        print(f'Loading original dataset from: {original_dataset_path}')
        with open(original_dataset_path, 'rb') as f:
            data_wrapper = pickle.load(f)

        # Load split info
        print(f'Loading split info from: {train_split_info_path}')
        with open(train_split_info_path, 'r') as f:
            split_info = json.load(f)

        train_indices = split_info.get('train_indices', [])
        test_indices = split_info.get('test_indices', [])

        print(f'Original images: {len(data_wrapper.images)}')
        print(f'Train indices: {len(train_indices)}')
        print(f'Test indices: {len(test_indices)}')

        # Define preprocessing transforms
        if normalize:
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,))  # [-1, 1] range
            ])
        else:
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),  # [0, 1] range
            ])

        def process_images(indices, name):
            images = []
            failed = 0
            
            print(f'Processing {name} images...')
            for idx in indices:
                try:
                    img_info = data_wrapper.images[idx]
                    if 'image_data' in img_info:
                        img_bytes = img_info['image_data']
                        pil_img = Image.open(io.BytesIO(img_bytes))
                        
                        # Convert to RGB if needed
                        if pil_img.mode != 'RGB':
                            pil_img = pil_img.convert('RGB')
                        
                        img_tensor = transform(pil_img)
                        images.append(img_tensor)
                        
                        if len(images) % 100 == 0:
                            print(f'  Processed {len(images)} {name} images...')
                            
                except Exception as e:
                    failed += 1
                    continue
            
            if images:
                images_tensor = torch.stack(images)
                print(f'  Successfully processed {len(images)} {name} images')
                print(f'  Failed: {failed}')
                print(f'  Tensor shape: {images_tensor.shape}')
                return images_tensor
            else:
                print(f'  No {name} images could be processed')
                return None

        # Process train and test images
        train_tensor = process_images(train_indices, 'train')
        test_tensor = process_images(test_indices, 'test')

        # Save processed images
        os.makedirs(os.path.dirname(train_images_path) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(test_images_path) or '.', exist_ok=True)

        with open(train_images_path, 'wb') as f:
            pickle.dump(train_tensor, f)
        print(f'Saved train images to: {train_images_path}')

        with open(test_images_path, 'wb') as f:
            pickle.dump(test_tensor, f)
        print(f'Saved test images to: {test_images_path}')

        print('='*80)
        print('PREPROCESSING COMPLETE')
        print('='*80)
        print(f'Train tensor shape: {train_tensor.shape if train_tensor is not None else \"None\"}')
        print(f'Test tensor shape: {test_tensor.shape if test_tensor is not None else \"None\"}')
        print('Image preprocessing completed successfully!')
        " "$0" "$1" "$2" "$3" "$4" "$5"
    args:
      - {inputPath: original_dataset}    
      - {inputPath: train_split_info}  
      - {inputValue: image_size}
      - {inputValue: normalize}
      - {outputPath: train_images}
      - {outputPath: test_images}
