name: Upload Preprocessed Dataset to CDN
description: Uploads preprocessed train and test datasets as .pkl files to CDN.
inputs:
  - name: train_data
    type: Dataset
    description: "Training data from Load Dataset component"
  - name: test_data
    type: Dataset
    description: "Test data from Load Dataset component"
  - name: preprocessed_data
    type: Dataset
    description: "Preprocessed data from Preprocess For GAN component"
  - name: dataset_info
    type: DatasetInfo
    description: "Dataset info from Load Dataset component"
  - name: bearer_token
    type: string
    description: "Access token from IAMLoginService component"
  - name: domain
    type: String
    description: "Domain for upload"
  - name: get_cdn
    type: String
    description: "CDN endpoint"
  - name: dataset_name
    type: String
    description: "Name for the dataset files"
    default: "gan_dataset"
outputs:
  - name: train_data_cdn_url
    type: String
    description: "CDN URL for uploaded train dataset .pkl file"
  - name: test_data_cdn_url
    type: String
    description: "CDN URL for uploaded test dataset .pkl file"
  - name: preprocessed_data_cdn_url
    type: String
    description: "CDN URL for uploaded preprocessed dataset .pkl file"
  - name: dataset_metadata
    type: String
    description: "Metadata about uploaded datasets"
implementation:
  container:
    image: python:3.8-slim
    command:
      - sh
      - -c
      - |
        if ! command -v curl &> /dev/null; then
            apt-get update > /dev/null && apt-get install -y curl > /dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, subprocess, json, pickle, tempfile, time, argparse
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data_path', type=str, required=True)
        parser.add_argument('--test_data_path', type=str, required=True)
        parser.add_argument('--preprocessed_data_path', type=str, required=True)
        parser.add_argument('--dataset_info_path', type=str, required=True)
        parser.add_argument('--bearer_token_path', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        parser.add_argument('--dataset_name', type=str, required=True)
        parser.add_argument('--train_data_cdn_url_path', type=str, required=True)
        parser.add_argument('--test_data_cdn_url_path', type=str, required=True)
        parser.add_argument('--preprocessed_data_cdn_url_path', type=str, required=True)
        parser.add_argument('--dataset_metadata_path', type=str, required=True)
        args = parser.parse_args()
        
        print('Starting Preprocessed Dataset CDN Upload')
        
        # Load bearer token
        with open(args.bearer_token_path, 'r') as f:
            bearer_token = f.read().strip()
        
        upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
        
        # Function to upload file to CDN
        def upload_file_to_cdn(file_path, filename):
            print(f'Uploading {filename}...')
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--fail",
                "--show-error",
                "--silent"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                response_json = json.loads(process.stdout)
                relative_cdn_url = response_json.get("cdnUrl", "")
                
                if not relative_cdn_url:
                    print('Error: No CDN URL in response')
                    return None
                
                full_url = f"{args.get_cdn}{relative_cdn_url}"
                print(f'Upload successful: {full_url}')
                return full_url
                
            except subprocess.CalledProcessError as e:
                print(f'Curl error: {e.returncode}')
                print(f'Error output: {e.stderr}')
                return None
            except json.JSONDecodeError as e:
                print(f'JSON parse error: {e}')
                return None
        
        # Load datasets
        with open(args.train_data_path, 'rb') as f:
            train_data = pickle.load(f)
        
        with open(args.test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        
        with open(args.preprocessed_data_path, 'rb') as f:
            preprocessed_data = pickle.load(f)
        
        with open(args.dataset_info_path, 'rb') as f:
            dataset_info = pickle.load(f)
        
        # Create dataset metadata
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        base_filename = f"{args.dataset_name}_{timestamp}"
        
        train_url = None
        test_url = None
        preprocessed_url = None
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # 1. Save train dataset as .pkl file
            train_file_path = os.path.join(temp_dir, f'{base_filename}_train.pkl')
            with open(train_file_path, 'wb') as f:
                pickle.dump({
                    'data': train_data,
                    'dataset_info': dataset_info,
                    'data_type': 'train',
                    'timestamp': timestamp,
                    'num_samples': len(train_data) if hasattr(train_data, '__len__') else 0
                }, f)
            
            print(f'Saved train dataset to: {train_file_path}')
            train_url = upload_file_to_cdn(train_file_path, f'{base_filename}_train.pkl')
            
            # 2. Save test dataset as .pkl file
            test_file_path = os.path.join(temp_dir, f'{base_filename}_test.pkl')
            with open(test_file_path, 'wb') as f:
                pickle.dump({
                    'data': test_data,
                    'dataset_info': dataset_info,
                    'data_type': 'test',
                    'timestamp': timestamp,
                    'num_samples': len(test_data) if hasattr(test_data, '__len__') else 0
                }, f)
            
            print(f'Saved test dataset to: {test_file_path}')
            test_url = upload_file_to_cdn(test_file_path, f'{base_filename}_test.pkl')
            
            # 3. Save preprocessed dataset as .pkl file
            preprocessed_file_path = os.path.join(temp_dir, f'{base_filename}_preprocessed.pkl')
            with open(preprocessed_file_path, 'wb') as f:
                pickle.dump({
                    'data': preprocessed_data,
                    'dataset_info': dataset_info,
                    'data_type': 'preprocessed',
                    'timestamp': timestamp,
                    'preprocessing_info': 'GAN-ready preprocessed data'
                }, f)
            
            print(f'Saved preprocessed dataset to: {preprocessed_file_path}')
            preprocessed_url = upload_file_to_cdn(preprocessed_file_path, f'{base_filename}_preprocessed.pkl')
        
        # Create dataset metadata
        dataset_metadata = {
            'dataset_name': args.dataset_name,
            'timestamp': timestamp,
            'files': {
                'train': f'{base_filename}_train.pkl',
                'test': f'{base_filename}_test.pkl',
                'preprocessed': f'{base_filename}_preprocessed.pkl'
            },
            'urls': {
                'train_url': train_url if train_url else '',
                'test_url': test_url if test_url else '',
                'preprocessed_url': preprocessed_url if preprocessed_url else ''
            },
            'statistics': {
                'train_samples': len(train_data) if hasattr(train_data, '__len__') else 0,
                'test_samples': len(test_data) if hasattr(test_data, '__len__') else 0,
                'dataset_info': dataset_info
            },
            'upload_status': {
                'train_uploaded': train_url is not None,
                'test_uploaded': test_url is not None,
                'preprocessed_uploaded': preprocessed_url is not None,
                'upload_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        
        # Save outputs
        os.makedirs(os.path.dirname(args.train_data_cdn_url_path) or '.', exist_ok=True)
        with open(args.train_data_cdn_url_path, 'w') as f:
            f.write(train_url if train_url else '')
        
        os.makedirs(os.path.dirname(args.test_data_cdn_url_path) or '.', exist_ok=True)
        with open(args.test_data_cdn_url_path, 'w') as f:
            f.write(test_url if test_url else '')
        
        os.makedirs(os.path.dirname(args.preprocessed_data_cdn_url_path) or '.', exist_ok=True)
        with open(args.preprocessed_data_cdn_url_path, 'w') as f:
            f.write(preprocessed_url if preprocessed_url else '')
        
        os.makedirs(os.path.dirname(args.dataset_metadata_path) or '.', exist_ok=True)
        with open(args.dataset_metadata_path, 'w') as f:
            json.dump(dataset_metadata, f, indent=2)
        
        print('Dataset CDN Upload Summary:')
        print(f'  Train dataset uploaded: {'Yes' if train_url else 'No'}')
        print(f'  Test dataset uploaded: {'Yes' if test_url else 'No'}')
        print(f'  Preprocessed dataset uploaded: {'Yes' if preprocessed_url else 'No'}')
        print(f'  Dataset metadata saved to: {args.dataset_metadata_path}')
    args:
      - --train_data_path
      - {inputPath: train_data}
      - --test_data_path
      - {inputPath: test_data}
      - --preprocessed_data_path
      - {inputPath: preprocessed_data}
      - --dataset_info_path
      - {inputPath: dataset_info}
      - --bearer_token_path
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --dataset_name
      - {inputValue: dataset_name}
      - --train_data_cdn_url_path
      - {outputPath: train_data_cdn_url}
      - --test_data_cdn_url_path
      - {outputPath: test_data_cdn_url}
      - --preprocessed_data_cdn_url_path
      - {outputPath: preprocessed_data_cdn_url}
      - --dataset_metadata_path
      - {outputPath: dataset_metadata}
