name: Evaluate DCGAN Model
description: Evaluates the trained DCGAN model with FID, Inception Score, and qualitative metrics.
inputs:
  - {name: trained_model, type: Model}
  - {name: test_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: eval_metrics, type: Metrics}
  - {name: eval_metrics_json, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v32
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import numpy as np
        from datetime import datetime
        import traceback

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--eval_metrics', type=str, required=True)
        parser.add_argument('--eval_metrics_json', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        print("="*60)
        print("Starting DCGAN Model Evaluation")
        print("="*60)

        # Load trained model
        with open(args.trained_model, 'rb') as f:
            model_wrapper = pickle.load(f)
        
        if 'dcgan' not in model_wrapper:
            raise ValueError("Invalid model format. Expected DCGAN wrapper.")
        
        dcgan = model_wrapper['dcgan']
        
        # Load test data
        with open(args.test_loader, 'rb') as f:
            test_data = pickle.load(f)
        
        print(f"Test Data Type: {type(test_data).__name__}")
        
        # Get evaluation parameters from config
        eval_sample_size = config.get('eval_sample_size', 1000)
        compute_fid = config.get('compute_fid', True)
        compute_inception_score = config.get('compute_inception_score', True)
        num_generated_samples = config.get('num_generated_samples', 1000)
        
        print(f"Evaluation Parameters:")
        print(f"  Sample Size: {eval_sample_size}")
        print(f"  Compute FID: {compute_fid}")
        print(f"  Compute Inception Score: {compute_inception_score}")
        print(f"  Generated Samples: {num_generated_samples}")
        
        # Initialize metrics dictionary
        metrics = {
            'evaluation_timestamp': datetime.now().isoformat(),
            'model_type': 'DCGAN',
            'training_method': model_wrapper.get('config', {}).get('training_algorithm', 'unknown'),
            'metrics': {}
        }
        
        try:
            # 1. Generate samples for evaluation
            print("1. Generating samples for evaluation...")
            with torch.no_grad():
                dcgan.trainer.generator.eval()
                
                # Generate in batches if needed
                all_generated = []
                batch_size = dcgan.config.batch_size
                
                for i in range(0, num_generated_samples, batch_size):
                    current_batch = min(batch_size, num_generated_samples - i)
                    z = torch.randn(current_batch, dcgan.config.latent_dim, 1, 1, 
                                  device=dcgan.device_manager.device)
                    generated = dcgan.trainer.generator(z)
                    all_generated.append(generated.cpu())
                    
                    if (i // batch_size) % 10 == 0:
                        print(f"  Generated {i + current_batch}/{num_generated_samples} samples")
                
                generated_images = torch.cat(all_generated, dim=0)
                print(f"  Generated {len(generated_images)} samples")
            
            # 2. Extract real images from test data
            print("2. Extracting real images from test data...")
            
            real_images = []
            if isinstance(test_data, torch.utils.data.DataLoader):
                for batch in test_data:
                    if isinstance(batch, (list, tuple)):
                        images = batch[0]  # Assume (images, labels) format
                    else:
                        images = batch
                    
                    real_images.append(images)
                    
                    if len(real_images) * images.size(0) >= eval_sample_size:
                        break
            
            elif isinstance(test_data, dict) and 'test_loader' in test_data:
                for batch in test_data['test_loader']:
                    if isinstance(batch, (list, tuple)):
                        images = batch[0]
                    else:
                        images = batch
                    
                    real_images.append(images)
                    
                    if len(real_images) * images.size(0) >= eval_sample_size:
                        break
            
            elif isinstance(test_data, list):
                real_images = test_data[:eval_sample_size]
                real_images = [img.unsqueeze(0) if img.dim() == 3 else img for img in real_images]
                real_images = torch.stack(real_images) if len(real_images) > 0 else torch.Tensor()
            
            if real_images:
                if isinstance(real_images, list):
                    real_images = torch.cat(real_images, dim=0)[:eval_sample_size]
                else:
                    real_images = real_images[:eval_sample_size]
                
                print(f"  Extracted {len(real_images)} real images")
            else:
                print("  Warning: Could not extract real images from test data")
                real_images = None
            
            # 3. Calculate metrics
            print("3. Calculating evaluation metrics...")
            
            # Qualitative metrics (always computed)
            if len(generated_images) > 0:
                # Basic statistics
                metrics['metrics']['generated_stats'] = {
                    'mean': float(generated_images.mean()),
                    'std': float(generated_images.std()),
                    'min': float(generated_images.min()),
                    'max': float(generated_images.max()),
                    'shape': list(generated_images.shape)
                }
                
                # Diversity metric (std of generated samples)
                pixel_std = generated_images.view(generated_images.size(0), -1).std(dim=0).mean()
                metrics['metrics']['pixel_diversity'] = float(pixel_std)
                
                print(f"  Pixel Diversity: {pixel_std:.4f}")
            
            # FID Score
            if compute_fid and real_images is not None and len(generated_images) >= 100:
                print("  Computing FID Score...")
                try:
                    from nesy_factory.GANs import compute_fid_score
                    
                    # Ensure images are on same device and properly formatted
                    real_for_fid = real_images.to(dcgan.device_manager.device)
                    gen_for_fid = generated_images.to(dcgan.device_manager.device)
                    
                    # Resize if needed
                    if real_for_fid.size(2) != 299 or real_for_fid.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        real_for_fid = torch.stack([resize(img) for img in real_for_fid])
                        gen_for_fid = torch.stack([resize(img) for img in gen_for_fid])
                    
                    fid_score = compute_fid_score(real_for_fid, gen_for_fid, dcgan.device_manager.device)
                    metrics['metrics']['fid_score'] = float(fid_score)
                    print(f"  FID Score: {fid_score:.2f}")
                    
                except Exception as e:
                    print(f"  Warning: FID computation failed: {e}")
                    metrics['metrics']['fid_error'] = str(e)
            
            # Inception Score
            if compute_inception_score and len(generated_images) >= 100:
                print("  Computing Inception Score...")
                try:
                    from nesy_factory.GANs import compute_inception_score
                    
                    gen_for_is = generated_images.to(dcgan.device_manager.device)
                    
                    # Resize if needed
                    if gen_for_is.size(2) != 299 or gen_for_is.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        gen_for_is = torch.stack([resize(img) for img in gen_for_is])
                    
                    is_mean, is_std = compute_inception_score(gen_for_is, dcgan.device_manager.device)
                    metrics['metrics']['inception_score'] = {
                        'mean': float(is_mean),
                        'std': float(is_std)
                    }
                    print(f"  Inception Score: {is_mean:.2f} ± {is_std:.2f}")
                    
                except Exception as e:
                    print(f"  Warning: Inception Score computation failed: {e}")
                    metrics['metrics']['inception_error'] = str(e)
            
            # Training history metrics
            if 'training_history' in model_wrapper:
                history = model_wrapper['training_history']
                metrics['training_summary'] = {
                    'final_d_loss': history.get('final_d_loss', 0),
                    'final_g_loss': history.get('final_g_loss', 0),
                    'total_epochs': history.get('total_epochs', 0),
                    'training_time': history.get('total_training_time', 0)
                }
            
            # Model configuration
            metrics['model_config'] = {
                'image_size': dcgan.config.image_size,
                'latent_dim': dcgan.config.latent_dim,
                'generator_layers': dcgan.config.generator_layers,
                'discriminator_layers': dcgan.config.discriminator_layers,
                'training_algorithm': dcgan.config.training_algorithm.value
            }
            
            print("\n" + "="*60)
            print("Evaluation Completed Successfully!")
            print("="*60)
            
            # Print summary
            print("Evaluation Summary:")
            if 'fid_score' in metrics['metrics']:
                print(f"  FID Score: {metrics['metrics']['fid_score']:.2f}")
            if 'inception_score' in metrics['metrics']:
                is_data = metrics['metrics']['inception_score']
                print(f"  Inception Score: {is_data['mean']:.2f} ± {is_data['std']:.2f}")
            if 'pixel_diversity' in metrics['metrics']:
                print(f"  Pixel Diversity: {metrics['metrics']['pixel_diversity']:.4f}")
            
            # Save metrics
            output_dir = os.path.dirname(args.eval_metrics)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(args.eval_metrics, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            with open(args.eval_metrics_json, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            print(f"Saved evaluation metrics to: {args.eval_metrics}")
            print(f"Saved JSON metrics to: {args.eval_metrics_json}")
            print("="*60)
            
        except Exception as e:
            print(f"Error during evaluation: {e}")
            traceback.print_exc()
            
            # Save error metrics
            error_metrics = {
                'error': str(e),
                'evaluation_timestamp': datetime.now().isoformat(),
                'status': 'failed'
            }
            
            with open(args.eval_metrics, 'w') as f:
                json.dump(error_metrics, f, indent=2)
            
            with open(args.eval_metrics_json, 'w') as f:
                json.dump(error_metrics, f, indent=2)
            
            raise
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_loader
      - {inputPath: test_loader}
      - --config
      - {inputValue: config}
      - --eval_metrics
      - {outputPath: eval_metrics}
      - --eval_metrics_json
      - {outputPath: eval_metrics_json}
