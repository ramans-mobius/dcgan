name: 2 Evaluate DCGAN Model
description: Evaluates the trained DCGAN model with FID, Inception Score, and qualitative metrics.
inputs:
  - {name: trained_model, type: Model}
  - {name: test_data, type: Dataset}
  - {name: config, type: String}
  - {name: debug, type: Boolean, optional: true, default: false, description: "Enable debug mode"}
  - {name: upload_results, type: Boolean, optional: true, default: false, description: "Upload evaluation results to CDN"}
  - {name: cdn_token, type: String, optional: true, description: "Bearer token for CDN upload"}
  - {name: cdn_domain, type: String, optional: true, description: "CDN domain for upload"}
outputs:
  - {name: eval_metrics, type: Metrics}
  - {name: eval_metrics_json, type: String}
  - {name: evaluation_images_url, type: String, optional: true, description: "URL to evaluation images if uploaded to CDN"}
  - {name: evaluation_debug, type: Data, optional: true, description: "Debug information and logs"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v32
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import numpy as np
        import pandas as pd
        from datetime import datetime
        import traceback
        import io
        from PIL import Image
        import requests
        from torchvision.utils import make_grid, save_image

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--debug', type=str, default='false')
        parser.add_argument('--upload_results', type=str, default='false')
        parser.add_argument('--cdn_token', type=str, default='')
        parser.add_argument('--cdn_domain', type=str, default='')
        parser.add_argument('--eval_metrics', type=str, required=True)
        parser.add_argument('--eval_metrics_json', type=str, required=True)
        parser.add_argument('--evaluation_images_url', type=str, required=True)
        parser.add_argument('--evaluation_debug', type=str, required=True)
        args = parser.parse_args()

        debug_mode = args.debug.lower() == 'true'
        upload_results = args.upload_results.lower() == 'true'
        config_dict = json.loads(args.config)

        print("Starting DCGAN Model Evaluation")

        debug_info = {
            'start_time': datetime.now().isoformat(),
            'model_path': args.trained_model,
            'test_data_path': args.test_data,
            'config_keys': list(config_dict.keys()),
            'steps': []
        }

        try:
            print("Loading trained model...")
            with open(args.trained_model, 'rb') as f:
                model_wrapper = pickle.load(f)
            
            if 'dcgan' not in model_wrapper:
                raise ValueError("Invalid model format. Expected DCGAN wrapper.")
            
            dcgan = model_wrapper['dcgan']
            training_history = model_wrapper.get('training_history', {})
            
            print(f"Loaded trained DCGAN model")
            print(f"Algorithm: {model_wrapper.get('config', {}).get('training_algorithm', 'unknown').upper()}")
            print(f"Training completed: {model_wrapper.get('trained', False)}")
            
            debug_info['steps'].append('model_loaded')
            debug_info['model_info'] = {
                'algorithm': model_wrapper.get('config', {}).get('training_algorithm', 'unknown'),
                'trained': model_wrapper.get('trained', False),
                'has_history': 'training_history' in model_wrapper
            }
            
        except Exception as e:
            error_msg = f"Failed to load model: {e}"
            print(error_msg)
            debug_info['error'] = error_msg
            debug_info['traceback'] = traceback.format_exc()
            raise

        print("Loading test data...")
        try:
            if args.test_data.endswith('.pkl') or args.test_data.endswith('.pickle'):
                with open(args.test_data, 'rb') as f:
                    test_data = pickle.load(f)
                
                if debug_mode:
                    print(f"DEBUG: Loaded pickle object of type: {type(test_data)}")
                
                if hasattr(test_data, 'images'):
                    print(f"Loaded DataWrapper with {len(test_data.images)} images")
                    debug_info['data_type'] = 'DataWrapper'
                    debug_info['data_info'] = {
                        'num_images': len(test_data.images),
                        'has_class_names': hasattr(test_data, 'class_names'),
                        'class_names': test_data.class_names if hasattr(test_data, 'class_names') else None
                    }
                    
                elif isinstance(test_data, pd.DataFrame):
                    print(f"Loaded DataFrame with shape: {test_data.shape}")
                    debug_info['data_type'] = 'DataFrame'
                    debug_info['data_info'] = {
                        'shape': test_data.shape,
                        'columns': list(test_data.columns)[:10]
                    }
                    
                else:
                    print(f"Unknown data type: {type(test_data)}")
                    debug_info['data_type'] = str(type(test_data))
                    
            elif args.test_data.endswith('.parquet'):
                test_data = pd.read_parquet(args.test_data)
                print(f"Loaded parquet file with shape: {test_data.shape}")
                debug_info['data_type'] = 'Parquet'
                debug_info['data_info'] = {'shape': test_data.shape}
                
            else:
                with open(args.test_data, 'r') as f:
                    content = f.read()
                    print(f"Loaded text file with {len(content)} characters")
                    debug_info['data_type'] = 'Text'
                
            debug_info['steps'].append('test_data_loaded')
            
        except Exception as e:
            error_msg = f"Failed to load test data: {e}"
            print(error_msg)
            debug_info['error'] = error_msg
            raise

        eval_sample_size = config_dict.get('eval_sample_size', 1000)
        compute_fid = config_dict.get('compute_fid', True)
        compute_inception_score = config_dict.get('compute_inception_score', True)
        num_generated_samples = config_dict.get('num_generated_samples', 1000)
        
        print(f"Evaluation Parameters:")
        print(f"  Sample Size: {eval_sample_size}")
        print(f"  Compute FID: {compute_fid}")
        print(f"  Compute Inception Score: {compute_inception_score}")
        print(f"  Generated Samples: {num_generated_samples}")

        metrics = {
            'evaluation_timestamp': datetime.now().isoformat(),
            'model_type': 'DCGAN',
            'training_method': model_wrapper.get('config', {}).get('training_algorithm', 'unknown'),
            'model_config': model_wrapper.get('config', {}),
            'training_summary': training_history if training_history else {},
            'metrics': {},
            'qualitative_analysis': {},
            'debug_info': debug_info if debug_mode else {}
        }

        try:
            print("Generating samples for evaluation...")
            try:
                with torch.no_grad():
                    dcgan.trainer.generator.eval()
                    
                    all_generated = []
                    batch_size = dcgan.config.batch_size
                    
                    for i in range(0, num_generated_samples, batch_size):
                        current_batch = min(batch_size, num_generated_samples - i)
                        z = torch.randn(current_batch, dcgan.config.latent_dim, 1, 1, 
                                      device=dcgan.device_manager.device)
                        generated = dcgan.trainer.generator(z)
                        all_generated.append(generated.cpu())
                        
                        if (i // batch_size) % 10 == 0:
                            print(f"  Generated {i + current_batch}/{num_generated_samples} samples")
                    
                    generated_images = torch.cat(all_generated, dim=0)
                    print(f"Generated {len(generated_images)} samples")
                    
                    debug_info['steps'].append('samples_generated')
                    debug_info['generated_samples'] = {
                        'count': len(generated_images),
                        'shape': list(generated_images.shape)
                    }
                    
            except Exception as e:
                error_msg = f"Failed to generate samples: {e}"
                print(error_msg)
                metrics['metrics']['generation_error'] = str(e)
                debug_info['steps'].append('samples_generation_failed')
                raise

            print("Saving generated samples...")
            try:
                sample_grid = make_grid(
                    generated_images[:64],
                    nrow=8,
                    normalize=True,
                    value_range=(-1, 1),
                    pad_value=1
                )
                
                local_sample_path = "/tmp/generated_samples_grid.png"
                save_image(sample_grid, local_sample_path)
                print(f"Saved sample grid to: {local_sample_path}")
                
                individual_dir = "/tmp/generated_samples"
                os.makedirs(individual_dir, exist_ok=True)
                
                for i in range(min(16, len(generated_images))):
                    sample_path = os.path.join(individual_dir, f"sample_{i:03d}.png")
                    save_image(generated_images[i], sample_path, normalize=True, value_range=(-1, 1))
                
                debug_info['steps'].append('samples_saved')
                debug_info['sample_paths'] = {
                    'grid': local_sample_path,
                    'individual_dir': individual_dir,
                    'individual_count': min(16, len(generated_images))
                }
                
            except Exception as e:
                print(f"Could not save samples: {e}")
                debug_info['steps'].append('samples_save_failed')

            print("Calculating basic statistics...")
            try:
                if len(generated_images) > 0:
                    gen_stats = {
                        'mean': float(generated_images.mean()),
                        'std': float(generated_images.std()),
                        'min': float(generated_images.min()),
                        'max': float(generated_images.max()),
                        'shape': list(generated_images.shape)
                    }
                    
                    pixel_std = generated_images.view(generated_images.size(0), -1).std(dim=0).mean()
                    gen_stats['pixel_diversity'] = float(pixel_std)
                    
                    metrics['metrics']['generated_stats'] = gen_stats
                    metrics['qualitative_analysis']['diversity_score'] = float(pixel_std)
                    
                    print(f"Pixel Diversity: {pixel_std:.4f}")
                    print(f"Mean: {gen_stats['mean']:.4f}, Std: {gen_stats['std']:.4f}")
                    
                    debug_info['steps'].append('basic_stats_calculated')
                    debug_info['basic_stats'] = gen_stats
                    
            except Exception as e:
                print(f"Could not calculate basic stats: {e}")
                metrics['metrics']['basic_stats_error'] = str(e)
                debug_info['steps'].append('basic_stats_failed')

            if compute_fid and len(generated_images) >= 100:
                print("Computing FID Score...")
                try:
                    from nesy_factory.GANs import compute_fid_score
                    
                    print("Note: Real images needed for FID calculation")
                    print("Using generated images as both real and fake for demonstration")
                    
                    real_for_fid = generated_images[:100].to(dcgan.device_manager.device)
                    fake_for_fid = generated_images[100:200].to(dcgan.device_manager.device)
                    
                    if real_for_fid.size(2) != 299 or real_for_fid.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        real_for_fid = torch.stack([resize(img) for img in real_for_fid])
                        fake_for_fid = torch.stack([resize(img) for img in fake_for_fid])
                    
                    fid_score = compute_fid_score(real_for_fid, fake_for_fid, dcgan.device_manager.device)
                    metrics['metrics']['fid_score'] = float(fid_score)
                    metrics['metrics']['fid_note'] = "Calculated with generated images as both real and fake (for demo)"
                    
                    print(f"FID Score (demo): {fid_score:.2f}")
                    
                    debug_info['steps'].append('fid_calculated')
                    debug_info['fid_info'] = {
                        'score': float(fid_score),
                        'note': 'Demo calculation'
                    }
                    
                except Exception as e:
                    print(f"FID computation failed: {e}")
                    metrics['metrics']['fid_error'] = str(e)
                    debug_info['steps'].append('fid_failed')

            if compute_inception_score and len(generated_images) >= 100:
                print("Computing Inception Score...")
                try:
                    from nesy_factory.GANs import compute_inception_score
                    
                    gen_for_is = generated_images[:100].to(dcgan.device_manager.device)
                    
                    if gen_for_is.size(2) != 299 or gen_for_is.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        gen_for_is = torch.stack([resize(img) for img in gen_for_is])
                    
                    is_mean, is_std = compute_inception_score(gen_for_is, dcgan.device_manager.device)
                    metrics['metrics']['inception_score'] = {
                        'mean': float(is_mean),
                        'std': float(is_std),
                        'interpretation': 'Higher is better (diverse and realistic images)'
                    }
                    
                    print(f"Inception Score: {is_mean:.2f} ± {is_std:.2f}")
                    
                    debug_info['steps'].append('inception_score_calculated')
                    debug_info['inception_score'] = {
                        'mean': float(is_mean),
                        'std': float(is_std)
                    }
                    
                except Exception as e:
                    print(f"Inception Score computation failed: {e}")
                    metrics['metrics']['inception_error'] = str(e)
                    debug_info['steps'].append('inception_score_failed')

            if upload_results and args.cdn_token and args.cdn_domain:
                print("Uploading results to CDN...")
                try:
                    upload_url = f"{args.cdn_domain}/mobius-content-service/v1.0/content/upload"
                    headers = {
                        'Authorization': f'Bearer {args.cdn_token}',
                    }
                    
                    with open(local_sample_path, 'rb') as f:
                        files = {
                            'file': ('dcgan_evaluation_samples.png', f, 'image/png')
                        }
                        data = {
                            'filePathAccess': 'private',
                            'filePath': '/dcgan/evaluation/'
                        }
                        
                        response = requests.post(upload_url, headers=headers, files=files, data=data)
                        
                        if response.status_code == 200:
                            response_json = response.json()
                            cdn_url = response_json.get('cdnUrl', '')
                            full_url = f"https://cdn-new.gov-cloud.ai{cdn_url}"
                            
                            print(f"Samples uploaded to CDN: {full_url}")
                            
                            with open(args.evaluation_images_url, 'w') as f:
                                f.write(full_url)
                            
                            metrics['cdn_url'] = full_url
                            debug_info['cdn_upload'] = {
                                'success': True,
                                'url': full_url
                            }
                        else:
                            print(f"Failed to upload to CDN: {response.status_code}")
                            debug_info['cdn_upload'] = {
                                'success': False,
                                'status_code': response.status_code
                            }
                            
                except Exception as e:
                    print(f"CDN upload failed: {e}")
                    debug_info['cdn_upload_error'] = str(e)

            print("Evaluation Completed Successfully!")
            
            output_dir = os.path.dirname(args.eval_metrics)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(args.eval_metrics, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            with open(args.eval_metrics_json, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            print(f"Saved evaluation metrics to: {args.eval_metrics}")
            print(f"Saved JSON metrics to: {args.eval_metrics_json}")
            
            debug_info['end_time'] = datetime.now().isoformat()
            debug_info['evaluation_completed'] = True
            
            with open(args.evaluation_debug, 'w') as f:
                json.dump(debug_info, f, indent=2)
            
            print(f"Saved debug info to: {args.evaluation_debug}")
            
            print("Evaluation Summary:")
            if 'fid_score' in metrics['metrics']:
                print(f"  FID Score: {metrics['metrics']['fid_score']:.2f}")
            if 'inception_score' in metrics['metrics']:
                is_data = metrics['metrics']['inception_score']
                print(f"  Inception Score: {is_data['mean']:.2f} ± {is_data['std']:.2f}")
            if 'pixel_diversity' in metrics.get('qualitative_analysis', {}):
                print(f"  Pixel Diversity: {metrics['qualitative_analysis']['diversity_score']:.4f}")
            
        except Exception as e:
            print(f"Error during evaluation: {e}")
            traceback.print_exc()
            
            error_metrics = {
                'error': str(e),
                'evaluation_timestamp': datetime.now().isoformat(),
                'status': 'failed',
                'debug_info': debug_info
            }
            
            with open(args.eval_metrics, 'w') as f:
                json.dump(error_metrics, f, indent=2)
            
            with open(args.eval_metrics_json, 'w') as f:
                json.dump(error_metrics, f, indent=2)
            
            debug_info['error'] = str(e)
            debug_info['traceback'] = traceback.format_exc()
            
            with open(args.evaluation_debug, 'w') as f:
                json.dump(debug_info, f, indent=2)
            
            raise
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_data
      - {inputPath: test_data}
      - --config
      - {inputValue: config}
      - --debug
      - {inputValue: debug}
      - --upload_results
      - {inputValue: upload_results}
      - --cdn_token
      - {inputValue: cdn_token}
      - --cdn_domain
      - {inputValue: cdn_domain}
      - --eval_metrics
      - {outputPath: eval_metrics}
      - --eval_metrics_json
      - {outputPath: eval_metrics_json}
      - --evaluation_images_url
      - {outputPath: evaluation_images_url}
      - --evaluation_debug
      - {outputPath: evaluation_debug}
