name: Evaluate DCGAN Model
description: Evaluates the trained DCGAN model with FID, Inception Score, and qualitative metrics.
inputs:
  - {name: trained_model, type: Model}
  - {name: test_data, type: Dataset}
  - {name: config, type: String}
  - {name: debug, type: Boolean, optional: true, default: false, description: "Enable debug mode"}
  - {name: upload_results, type: Boolean, optional: true, default: false, description: "Upload evaluation results to CDN"}
  - {name: cdn_token, type: String, optional: true, description: "Bearer token for CDN upload"}
  - {name: cdn_domain, type: String, optional: true, description: "CDN domain for upload"}
outputs:
  - {name: eval_metrics, type: Metrics}
  - {name: eval_metrics_json, type: String}
  - {name: evaluation_images_url, type: String, optional: true, description: "URL to evaluation images if uploaded to CDN"}
  - {name: evaluation_debug, type: Data, optional: true, description: "Debug information and logs"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v32
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import numpy as np
        import pandas as pd
        from datetime import datetime
        import traceback
        import io
        from PIL import Image
        import requests
        import base64
        from torchvision.utils import make_grid, save_image

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--debug', type=str, default='false')
        parser.add_argument('--upload_results', type=str, default='false')
        parser.add_argument('--cdn_token', type=str, default='')
        parser.add_argument('--cdn_domain', type=str, default='')
        parser.add_argument('--eval_metrics', type=str, required=True)
        parser.add_argument('--eval_metrics_json', type=str, required=True)
        parser.add_argument('--evaluation_images_url', type=str, required=True)
        parser.add_argument('--evaluation_debug', type=str, required=True)
        args = parser.parse_args()

        debug_mode = args.debug.lower() == 'true'
        upload_results = args.upload_results.lower() == 'true'
        
        config_dict = json.loads(args.config)

        print("="*60)
        print("Starting DCGAN Model Evaluation")
        print("="*60)

        # Initialize debug info
        debug_info = {
            'start_time': datetime.now().isoformat(),
            'model_path': args.trained_model,
            'test_data_path': args.test_data,
            'config_keys': list(config_dict.keys()),
            'steps': []
        }

        # Load trained model
        try:
            print("Loading trained model...")
            with open(args.trained_model, 'rb') as f:
                model_wrapper = pickle.load(f)
            
            if 'dcgan' not in model_wrapper:
                raise ValueError("Invalid model format. Expected DCGAN wrapper.")
            
            dcgan = model_wrapper['dcgan']
            training_history = model_wrapper.get('training_history', {})
            
            print(f"Loaded trained DCGAN model")
            print(f"   Algorithm: {model_wrapper.get('config', {}).get('training_algorithm', 'unknown').upper()}")
            print(f"   Training completed: {model_wrapper.get('trained', False)}")
            
            debug_info['steps'].append('model_loaded')
            debug_info['model_info'] = {
                'algorithm': model_wrapper.get('config', {}).get('training_algorithm', 'unknown'),
                'trained': model_wrapper.get('trained', False),
                'has_history': 'training_history' in model_wrapper
            }
            
        except Exception as e:
            error_msg = f" Failed to load model: {e}"
            print(error_msg)
            debug_info['error'] = error_msg
            debug_info['traceback'] = traceback.format_exc()
            raise

        # Load test data
        print("ðŸ“Š Loading test data...")
        try:
            # Check file type and load accordingly
            if args.test_data.endswith('.pkl') or args.test_data.endswith('.pickle'):
                with open(args.test_data, 'rb') as f:
                    test_data = pickle.load(f)
                
                if debug_mode:
                    print(f"DEBUG: Loaded pickle object of type: {type(test_data)}")
                    if hasattr(test_data, '__dict__'):
                        print(f"DEBUG: Object attributes: {list(test_data.__dict__.keys())[:10]}")
                
                # Handle different data formats
                if hasattr(test_data, 'images'):
                    print(f" Loaded DataWrapper with {len(test_data.images)} images")
                    debug_info['data_type'] = 'DataWrapper'
                    debug_info['data_info'] = {
                        'num_images': len(test_data.images),
                        'has_class_names': hasattr(test_data, 'class_names'),
                        'class_names': test_data.class_names if hasattr(test_data, 'class_names') else None
                    }
                    
                elif isinstance(test_data, pd.DataFrame):
                    print(f" Loaded DataFrame with shape: {test_data.shape}")
                    debug_info['data_type'] = 'DataFrame'
                    debug_info['data_info'] = {
                        'shape': test_data.shape,
                        'columns': list(test_data.columns)[:10]
                    }
                    
                else:
                    print(f"  Unknown data type: {type(test_data)}")
                    debug_info['data_type'] = str(type(test_data))
                    
            elif args.test_data.endswith('.parquet'):
                test_data = pd.read_parquet(args.test_data)
                print(f" Loaded parquet file with shape: {test_data.shape}")
                debug_info['data_type'] = 'Parquet'
                debug_info['data_info'] = {'shape': test_data.shape}
                
            else:
                # Try to load as text/JSON
                with open(args.test_data, 'r') as f:
                    content = f.read()
                    print(f" Loaded text file with {len(content)} characters")
                    debug_info['data_type'] = 'Text'
                
            debug_info['steps'].append('test_data_loaded')
            
        except Exception as e:
            error_msg = f" Failed to load test data: {e}"
            print(error_msg)
            debug_info['error'] = error_msg
            raise

        # Get evaluation parameters
        eval_sample_size = config_dict.get('eval_sample_size', 1000)
        compute_fid = config_dict.get('compute_fid', True)
        compute_inception_score = config_dict.get('compute_inception_score', True)
        num_generated_samples = config_dict.get('num_generated_samples', 1000)
        
        print(f"Evaluation Parameters:")
        print(f"  Sample Size: {eval_sample_size}")
        print(f"  Compute FID: {compute_fid}")
        print(f"  Compute Inception Score: {compute_inception_score}")
        print(f"  Generated Samples: {num_generated_samples}")

        # Initialize metrics dictionary
        metrics = {
            'evaluation_timestamp': datetime.now().isoformat(),
            'model_type': 'DCGAN',
            'training_method': model_wrapper.get('config', {}).get('training_algorithm', 'unknown'),
            'model_config': model_wrapper.get('config', {}),
            'training_summary': training_history if training_history else {},
            'metrics': {},
            'qualitative_analysis': {},
            'debug_info': debug_info if debug_mode else {}
        }

        try:
            # 1. Generate samples for evaluation
            print("1.  Generating samples for evaluation...")
            try:
                with torch.no_grad():
                    dcgan.trainer.generator.eval()
                    
                    # Generate in batches
                    all_generated = []
                    batch_size = dcgan.config.batch_size
                    
                    for i in range(0, num_generated_samples, batch_size):
                        current_batch = min(batch_size, num_generated_samples - i)
                        z = torch.randn(current_batch, dcgan.config.latent_dim, 1, 1, 
                                      device=dcgan.device_manager.device)
                        generated = dcgan.trainer.generator(z)
                        all_generated.append(generated.cpu())
                        
                        if (i // batch_size) % 10 == 0:
                            print(f"  Generated {i + current_batch}/{num_generated_samples} samples")
                    
                    generated_images = torch.cat(all_generated, dim=0)
                    print(f"   Generated {len(generated_images)} samples")
                    
                    debug_info['steps'].append('samples_generated')
                    debug_info['generated_samples'] = {
                        'count': len(generated_images),
                        'shape': list(generated_images.shape)
                    }
                    
            except Exception as e:
                error_msg = f" Failed to generate samples: {e}"
                print(error_msg)
                metrics['metrics']['generation_error'] = str(e)
                debug_info['steps'].append('samples_generation_failed')
                raise

            # 2. Save generated samples as images
            print("2.  Saving generated samples...")
            try:
                # Create a grid of samples
                sample_grid = make_grid(
                    generated_images[:64],  # First 64 samples
                    nrow=8,
                    normalize=True,
                    value_range=(-1, 1),
                    pad_value=1
                )
                
                # Save locally
                local_sample_path = "/tmp/generated_samples_grid.png"
                save_image(sample_grid, local_sample_path)
                print(f"   Saved sample grid to: {local_sample_path}")
                
                # Also save individual samples
                individual_dir = "/tmp/generated_samples"
                os.makedirs(individual_dir, exist_ok=True)
                
                for i in range(min(16, len(generated_images))):
                    sample_path = os.path.join(individual_dir, f"sample_{i:03d}.png")
                    save_image(generated_images[i], sample_path, normalize=True, value_range=(-1, 1))
                
                debug_info['steps'].append('samples_saved')
                debug_info['sample_paths'] = {
                    'grid': local_sample_path,
                    'individual_dir': individual_dir,
                    'individual_count': min(16, len(generated_images))
                }
                
            except Exception as e:
                print(f"    Could not save samples: {e}")
                debug_info['steps'].append('samples_save_failed')

            # 3. Calculate basic statistics
            print("3.  Calculating basic statistics...")
            try:
                if len(generated_images) > 0:
                    # Basic statistics
                    gen_stats = {
                        'mean': float(generated_images.mean()),
                        'std': float(generated_images.std()),
                        'min': float(generated_images.min()),
                        'max': float(generated_images.max()),
                        'shape': list(generated_images.shape)
                    }
                    
                    # Diversity metric (std of generated samples)
                    pixel_std = generated_images.view(generated_images.size(0), -1).std(dim=0).mean()
                    gen_stats['pixel_diversity'] = float(pixel_std)
                    
                    metrics['metrics']['generated_stats'] = gen_stats
                    metrics['qualitative_analysis']['diversity_score'] = float(pixel_std)
                    
                    print(f"   Pixel Diversity: {pixel_std:.4f}")
                    print(f"   Mean: {gen_stats['mean']:.4f}, Std: {gen_stats['std']:.4f}")
                    
                    debug_info['steps'].append('basic_stats_calculated')
                    debug_info['basic_stats'] = gen_stats
                    
            except Exception as e:
                print(f"    Could not calculate basic stats: {e}")
                metrics['metrics']['basic_stats_error'] = str(e)
                debug_info['steps'].append('basic_stats_failed')

            # 4. Calculate FID Score (if enabled and possible)
            if compute_fid and len(generated_images) >= 100:
                print("4.  Computing FID Score...")
                try:
                    from nesy_factory.GANs import compute_fid_score
                    
                    # For FID, we need real images for comparison
                    # In your pipeline, you might need to extract real images from test data
                    print("    Note: Real images needed for FID calculation")
                    print("    Using generated images as both real and fake for demonstration")
                    
                    # This is a placeholder - in reality, you'd load real images
                    real_for_fid = generated_images[:100].to(dcgan.device_manager.device)
                    fake_for_fid = generated_images[100:200].to(dcgan.device_manager.device)
                    
                    # Resize to 299x299 if needed
                    if real_for_fid.size(2) != 299 or real_for_fid.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        real_for_fid = torch.stack([resize(img) for img in real_for_fid])
                        fake_for_fid = torch.stack([resize(img) for img in fake_for_fid])
                    
                    fid_score = compute_fid_score(real_for_fid, fake_for_fid, dcgan.device_manager.device)
                    metrics['metrics']['fid_score'] = float(fid_score)
                    metrics['metrics']['fid_note'] = "Calculated with generated images as both real and fake (for demo)"
                    
                    print(f"   FID Score (demo): {fid_score:.2f}")
                    
                    debug_info['steps'].append('fid_calculated')
                    debug_info['fid_info'] = {
                        'score': float(fid_score),
                        'note': 'Demo calculation'
                    }
                    
                except Exception as e:
                    print(f"    FID computation failed: {e}")
                    metrics['metrics']['fid_error'] = str(e)
                    debug_info['steps'].append('fid_failed')

            # 5. Calculate Inception Score (if enabled)
            if compute_inception_score and len(generated_images) >= 100:
                print("5.  Computing Inception Score...")
                try:
                    from nesy_factory.GANs import compute_inception_score
                    
                    gen_for_is = generated_images[:100].to(dcgan.device_manager.device)
                    
                    # Resize if needed
                    if gen_for_is.size(2) != 299 or gen_for_is.size(3) != 299:
                        from torchvision import transforms
                        resize = transforms.Resize((299, 299))
                        gen_for_is = torch.stack([resize
