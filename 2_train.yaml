name: ssss Train GAN Model Enhanced Fixed
description: Enhanced GAN trainer using nesy_factory GAN implementations.
inputs:
  - name: initialized_model
    type: Model
    description: "Initialized model from Initialize GAN Model component"
  - name: preprocessed_data
    type: Dataset
    description: "Processed data from Preprocess For GAN component"
  - name: gan_config
    type: String
    description: "GAN configuration from Preprocess For GAN component"
  - name: train_config
    type: String
    description: "Training configuration parameters"
outputs:
  - name: trained_model
    type: Model
    description: "Trained GAN model"
  - name: training_history
    type: String
    description: "Training metrics history JSON"
  - name: generated_samples
    type: Dataset
    description: "Generated sample images during training"
  - name: training_metrics
    type: String
    description: "Training evaluation metrics including FID/IS scores"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v34
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, base64, io, time, warnings, traceback
        import numpy as np
        
        warnings.filterwarnings('ignore')
        
        print('Starting GAN Training with nesy_factory')
        
        # Parse arguments
        initialized_model_path = sys.argv[1]
        preprocessed_data_path = sys.argv[2]
        gan_config_path = sys.argv[3]
        train_config_str = sys.argv[4]
        trained_model_path = sys.argv[5]
        training_history_path = sys.argv[6]
        generated_samples_path = sys.argv[7]
        training_metrics_path = sys.argv[8]
        
        start_time = time.time()
        
        # Load initialized model
        try:
            with open(initialized_model_path, 'rb') as f:
                gan_model = pickle.load(f)
            print(f'Model loaded successfully: {type(gan_model).__name__}')
        except Exception as e:
            print(f'Error loading model: {e}')
            raise
        
        # Load preprocessed data
        try:
            with open(preprocessed_data_path, 'rb') as f:
                data_wrapper = pickle.load(f)
            print(f'Preprocessed data loaded: {type(data_wrapper)}')
        except Exception as e:
            print(f'Error loading preprocessed data: {e}')
            raise
        
        # Load configurations
        try:
            with open(gan_config_path, 'r') as f:
                gan_config = json.load(f)
            print('GAN config loaded successfully')
        except Exception as e:
            print(f'Error loading GAN config: {e}')
            gan_config = {}
        
        try:
            train_config = json.loads(train_config_str) if train_config_str and train_config_str.strip() else {}
            print('Train config parsed successfully')
        except json.JSONDecodeError as e:
            print(f'Error parsing train config: {e}, using defaults')
            train_config = {}
        
        # Import torch and nesy_factory
        try:
            import torch
            from torch.utils.data import DataLoader
            print(f'PyTorch version: {torch.__version__}')
            print(f'CUDA available: {torch.cuda.is_available()}')
            
            # Try to import nesy_factory evaluation functions
            try:
                from nesy_factory.GANs import compute_fid_score, compute_inception_score
                print('Imported nesy_factory evaluation functions')
            except ImportError as e:
                print(f'Note: nesy_factory evaluation functions not available: {e}')
                compute_fid_score = None
                compute_inception_score = None
                
        except ImportError as e:
            print(f'PyTorch import error: {e}')
            raise
        
        # Extract data from wrapper
        dataset = None
        if isinstance(data_wrapper, dict) and 'dataset' in data_wrapper:
            dataset = data_wrapper['dataset']
            print(f'Using wrapped dataset with {len(dataset)} samples')
        
        if dataset is None:
            print('No valid dataset found, creating dummy dataset')
            import torch
            dummy_data = torch.randn(100, 3, 64, 64)  # 100 samples, 3 channels, 64x64
            from torch.utils.data import TensorDataset
            dataset = TensorDataset(dummy_data)
        
        # Extract training parameters
        epochs = train_config.get('epochs', 5)
        batch_size = train_config.get('batch_size', 32)
        sample_interval = train_config.get('sample_interval', 2)
        save_interval = train_config.get('save_interval', 5)
        log_interval = train_config.get('log_interval', 1)
        
        print(f'Training Configuration:')
        print(f'  Epochs: {epochs}')
        print(f'  Batch Size: {batch_size}')
        print(f'  Sample Interval: {sample_interval}')
        
        # Create data loader
        train_loader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0
        )
        
        print(f'Created data loader with {len(train_loader)} batches')
        
        # Train the model using nesy_factory's train method
        training_results = None
        
        try:
            # Check if model has train() method (nesy_factory VanillaGAN or DCGAN)
            if hasattr(gan_model, 'train'):
                print('Model has train() method, starting training...')
                
                # Call the model's train method
                training_results = gan_model.train(
                    dataloader=train_loader,
                    epochs=epochs,
                    save_dir='/tmp/gan_output',
                    save_interval=save_interval,
                    sample_interval=sample_interval
                )
                
                print('Training completed successfully')
                
            # For Forward-Forward or CAFO trainers that need special handling
            elif hasattr(gan_model, 'trainer'):
                print('Model has trainer, checking training algorithm...')
                
                model_type = gan_config.get('model_type', 'dcgan')
                training_algorithm = gan_config.get('training_algorithm', 'backprop')
                
                # Handle Forward-Forward training
                if training_algorithm == 'forward_forward' and hasattr(gan_model.trainer, 'train_forward_forward'):
                    print('Starting Forward-Forward training...')
                    ff_results = gan_model.trainer.train_forward_forward(
                        dataloader=train_loader,
                        verbose=True
                    )
                    
                    # Then do regular training
                    print('Starting main training after FF...')
                    for epoch in range(epochs):
                        metrics = gan_model.trainer.train_epoch(train_loader)
                        if epoch % log_interval == 0:
                            print(f'Epoch [{epoch+1}/{epochs}] - {metrics}')
                    
                    training_results = {
                        'epochs_completed': epochs,
                        'ff_results': ff_results,
                        'training_success': True
                    }
                    
                # Handle CAFO training
                elif training_algorithm == 'cafo' and hasattr(gan_model.trainer, 'train_cafo'):
                    print('Starting CAFO training...')
                    cafo_results = gan_model.trainer.train_cafo(
                        dataloader=train_loader,
                        verbose=True
                    )
                    
                    # Then do regular training
                    print('Starting main training after CAFO...')
                    for epoch in range(epochs):
                        metrics = gan_model.trainer.train_epoch(train_loader)
                        if epoch % log_interval == 0:
                            print(f'Epoch [{epoch+1}/{epochs}] - {metrics}')
                    
                    training_results = {
                        'epochs_completed': epochs,
                        'cafo_results': cafo_results,
                        'training_success': True
                    }
                    
                # Standard backprop training
                else:
                    print('Starting standard training...')
                    all_metrics = []
                    for epoch in range(epochs):
                        metrics = gan_model.trainer.train_epoch(train_loader)
                        all_metrics.append(metrics)
                        
                        if epoch % log_interval == 0:
                            print(f'Epoch [{epoch+1}/{epochs}] - {metrics}')
                    
                    training_results = {
                        'epochs_completed': epochs,
                        'training_history': all_metrics,
                        'final_metrics': all_metrics[-1] if all_metrics else {},
                        'training_success': True
                    }
                    
            else:
                raise AttributeError("Model doesn't have train() method or trainer")
                
        except Exception as e:
            print(f'Training error: {e}')
            traceback.print_exc()
            
            training_results = {
                'epochs_completed': epochs,
                'error': str(e),
                'training_success': False
            }
        
        # Generate final samples using nesy_factory's generate method
        generated_samples = []
        try:
            if hasattr(gan_model, 'generate'):
                num_samples = 16
                samples = gan_model.generate(num_samples)
                
                # Convert samples to base64 for storage
                from PIL import Image
                import torchvision.transforms as transforms
                
                for i in range(min(num_samples, len(samples))):
                    sample = samples[i].detach().cpu()
                    
                    # Normalize from [-1, 1] to [0, 1]
                    sample = (sample + 1) / 2
                    
                    # Convert to PIL Image
                    if sample.shape[0] == 1:  # Grayscale
                        img_np = sample.squeeze(0).numpy()
                        img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='L')
                    else:  # RGB
                        img_np = sample.permute(1, 2, 0).numpy()
                        img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='RGB')
                    
                    # Convert to base64
                    img_bytes = io.BytesIO()
                    img_pil.save(img_bytes, format='PNG')
                    base64_data = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
                    
                    generated_samples.append({
                        'epoch': epochs,
                        'sample_index': i,
                        'image_data': base64_data,
                        'filename': f'gan_sample_{i}.png',
                        'model_type': gan_config.get('model_type', 'unknown')
                    })
                
                print(f'Generated {len(generated_samples)} sample images')
            else:
                print('Model doesn\'t have generate() method')
                
        except Exception as e:
            print(f'Error generating samples: {e}')
        
        # Create training history
        training_history = {
            'model_type': gan_config.get('model_type', 'unknown'),
            'epochs': epochs,
            'batch_size': batch_size,
            'training_success': training_results.get('training_success', False) if training_results else False,
            'final_metrics': training_results.get('final_metrics', {}) if training_results else {},
            'training_time_seconds': time.time() - start_time,
            'generated_samples_count': len(generated_samples)
        }
        
        # Add training history if available
        if training_results and 'training_history' in training_results:
            training_history['history'] = training_results['training_history']
        
        # Create training metrics
        training_metrics_data = {
            'model_type': gan_config.get('model_type', 'unknown'),
            'training_completed': training_results.get('training_success', False) if training_results else False,
            'total_epochs': epochs,
            'final_metrics': training_results.get('final_metrics', {}) if training_results else {},
            'samples_generated': len(generated_samples),
            'training_time_seconds': time.time() - start_time,
            'batch_size': batch_size,
            'dataset_size': len(dataset)
        }
        
        # Save outputs
        os.makedirs(os.path.dirname(trained_model_path) or '.', exist_ok=True)
        with open(trained_model_path, 'wb') as f:
            pickle.dump(gan_model, f)
        
        os.makedirs(os.path.dirname(training_history_path) or '.', exist_ok=True)
        with open(training_history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        
        os.makedirs(os.path.dirname(generated_samples_path) or '.', exist_ok=True)
        with open(generated_samples_path, 'wb') as f:
            pickle.dump(generated_samples, f)
        
        os.makedirs(os.path.dirname(training_metrics_path) or '.', exist_ok=True)
        with open(training_metrics_path, 'w') as f:
            json.dump(training_metrics_data, f, indent=2)
        
        print('Training outputs saved successfully')
        print(f'  Trained model: {trained_model_path}')
        print(f'  Training history: {training_history_path}')
        print(f'  Generated samples: {len(generated_samples)} images')
        print(f'  Training metrics: {training_metrics_path}')
        print(f'  Total training time: {time.time() - start_time:.2f} seconds')
    args:
      - {inputPath: initialized_model}
      - {inputPath: preprocessed_data}
      - {inputPath: gan_config}
      - {inputValue: train_config}
      - {outputPath: trained_model}
      - {outputPath: training_history}
      - {outputPath: generated_samples}
      - {outputPath: training_metrics}
