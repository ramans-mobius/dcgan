name: Create GAN Continual Learning Tasks
description: Splits GAN training into multiple continual learning tasks for progressive training.
inputs:
  - name: train_data
    type: Dataset
    description: "Training data from Load Dataset component"
  - name: test_data
    type: Dataset
    description: "Test data from Load Dataset component"
  - name: model_config
    type: String
    description: "Model configuration specifying GAN type and algorithm"
  - name: task_config
    type: String
    description: "Continual learning task configuration"
outputs:
  - name: gan_tasks
    type: Dataset
    description: "List of GAN continual learning tasks"
  - name: task_summary
    type: String
    description: "Summary of created tasks"
  - name: progressive_configs
    type: String
    description: "Progressive GAN configurations for each task"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v33
    command:
      - sh
      - -c
      - |
        pip install torchvision==0.15.2 --no-deps --quiet
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, base64, io, math, argparse
        import numpy as np
        import torch
        from torch.utils.data import DataLoader, Dataset, TensorDataset
        from PIL import Image
        import torchvision.transforms as transforms
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data_path', type=str, required=True)
        parser.add_argument('--test_data_path', type=str, required=True)
        parser.add_argument('--model_config_str', type=str, required=True)
        parser.add_argument('--task_config_str', type=str, required=True)
        parser.add_argument('--gan_tasks_path', type=str, required=True)
        parser.add_argument('--task_summary_path', type=str, required=True)
        parser.add_argument('--progressive_configs_path', type=str, required=True)
        args = parser.parse_args()
        
        print('Creating GAN Continual Learning Tasks')
        
        # Load data
        with open(args.train_data_path, 'rb') as f:
            train_data = pickle.load(f)
        
        with open(args.test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        
        # Parse configurations
        model_config = json.loads(args.model_config_str) if args.model_config_str else {}
        task_config = json.loads(args.task_config_str) if args.task_config_str else {}
        
        # Extract configurations
        model_type = model_config.get('model_type', 'dcgan').lower()
        training_algorithm = model_config.get('training_algorithm', 'backprop').lower()
        
        # Task parameters
        num_tasks = task_config.get('num_tasks', 3)
        progressive_resolution = task_config.get('progressive_resolution', True)
        incremental_complexity = task_config.get('incremental_complexity', True)
        task_type = task_config.get('task_type', 'temporal_split')
        
        print(f'Model Type: {model_type}')
        print(f'Training Algorithm: {training_algorithm}')
        print(f'Number of Tasks: {num_tasks}')
        print(f'Task Type: {task_type}')
        
        # Prepare data for splitting
        def prepare_dataset(data_list):
            prepared_data = []
            for item in data_list:
                if isinstance(item, dict) and 'image_data' in item:
                    try:
                        img_data = base64.b64decode(item['image_data'])
                        img = Image.open(io.BytesIO(img_data))
                        
                        # Basic transform for analysis
                        transform = transforms.Compose([
                            transforms.Resize((64, 64)),
                            transforms.ToTensor(),
                            transforms.Normalize((0.5,), (0.5,))
                        ])
                        
                        img_tensor = transform(img)
                        prepared_data.append(img_tensor)
                    except:
                        continue
                elif isinstance(item, torch.Tensor):
                    prepared_data.append(item)
            
            if prepared_data:
                return torch.stack(prepared_data)
            return torch.tensor([])
        
        # Prepare datasets
        train_tensors = prepare_dataset(train_data)
        test_tensors = prepare_dataset(test_data)
        
        print(f'Train samples: {len(train_tensors)}')
        print(f'Test samples: {len(test_tensors)}')
        
        # Create GAN tasks based on strategy
        gan_tasks = []
        progressive_configs = []
        
        if task_type == 'temporal_split':
            # Split data temporally
            train_splits = np.array_split(range(len(train_tensors)), num_tasks)
            test_splits = np.array_split(range(len(test_tensors)), num_tasks)
            
            for task_idx in range(num_tasks):
                # Select data for this task
                train_indices = train_splits[task_idx]
                test_indices = test_splits[task_idx]
                
                task_train_data = train_tensors[train_indices] if len(train_tensors) > 0 else torch.tensor([])
                task_test_data = test_tensors[test_indices] if len(test_tensors) > 0 else torch.tensor([])
                
                # Create progressive configuration for this task
                if progressive_resolution:
                    # Progressive resolution: start with small images, increase size
                    base_size = model_config.get('image_size', 64)
                    progressive_size = min(base_size, 32 * (task_idx + 1))
                    
                    task_config_dict = {
                        'task_id': task_idx,
                        'model_type': model_type,
                        'training_algorithm': training_algorithm,
                        'image_size': progressive_size,
                        'latent_dim': model_config.get('latent_dim', 100),
                        'generator_layers': model_config.get('generator_layers', []),
                        'discriminator_layers': model_config.get('discriminator_layers', []),
                        'batch_size': max(8, model_config.get('batch_size', 32) // (num_tasks - task_idx)),
                        'learning_rate': model_config.get('learning_rate', 0.0002),
                        'epochs': task_config.get('epochs_per_task', 10),
                        'description': f'Task {task_idx+1}: Temporal Period {task_idx+1}/{num_tasks}',
                        'progressive': {
                            'resolution': progressive_size,
                            'complexity_level': task_idx + 1,
                            'is_final_task': task_idx == num_tasks - 1
                        }
                    }
                else:
                    # Same configuration for all tasks
                    task_config_dict = {
                        'task_id': task_idx,
                        'model_type': model_type,
                        'training_algorithm': training_algorithm,
                        'image_size': model_config.get('image_size', 64),
                        'latent_dim': model_config.get('latent_dim', 100),
                        'batch_size': model_config.get('batch_size', 32),
                        'learning_rate': model_config.get('learning_rate', 0.0002),
                        'epochs': task_config.get('epochs_per_task', 10),
                        'description': f'Task {task_idx+1}: Temporal Period {task_idx+1}/{num_tasks}'
                    }
                
                # Adjust architecture for progressive complexity
                if incremental_complexity and model_type == 'dcgan':
                    # Increase layer complexity with each task
                    base_layers = 3
                    current_layers = min(6, base_layers + task_idx)
                    
                    task_config_dict['generator_layers'] = [512 // (2 ** i) for i in range(current_layers)]
                    task_config_dict['discriminator_layers'] = [64 * (2 ** i) for i in range(current_layers - 1)]
                
                # Create task dataset wrapper
                task_data_wrapper = {
                    'task_id': task_idx,
                    'train_data': task_train_data,
                    'test_data': task_test_data,
                    'train_indices': train_indices.tolist(),
                    'test_indices': test_indices.tolist(),
                    'num_train_samples': len(train_indices),
                    'num_test_samples': len(test_indices),
                    'task_config': task_config_dict,
                    'requires_preprocessing': True
                }
                
                gan_tasks.append(task_data_wrapper)
                progressive_configs.append(task_config_dict)
                
                print(f'Created Task {task_idx+1}: {len(train_indices)} train, {len(test_indices)} test samples')
        
        elif task_type == 'progressive_difficulty':
            # Progressive difficulty: easier samples first
            # Sort by image complexity (simplified: by variance)
            if len(train_tensors) > 0:
                variances = [img.var().item() for img in train_tensors]
                sorted_indices = np.argsort(variances)
                
                train_splits = np.array_split(sorted_indices, num_tasks)
                
                for task_idx in range(num_tasks):
                    task_indices = train_splits[task_idx]
                    task_train_data = train_tensors[task_indices]
                    
                    # Create configuration with increasing difficulty
                    difficulty_factor = (task_idx + 1) / num_tasks
                    
                    task_config_dict = {
                        'task_id': task_idx,
                        'model_type': model_type,
                        'training_algorithm': training_algorithm,
                        'image_size': model_config.get('image_size', 64),
                        'latent_dim': max(50, int(model_config.get('latent_dim', 100) * difficulty_factor)),
                        'batch_size': model_config.get('batch_size', 32),
                        'learning_rate': model_config.get('learning_rate', 0.0002) * (1.0 / (task_idx + 1)),
                        'epochs': max(5, task_config.get('epochs_per_task', 10) + task_idx * 2),
                        'description': f'Task {task_idx+1}: Difficulty Level {task_idx+1}/{num_tasks}',
                        'difficulty_factor': difficulty_factor
                    }
                    
                    # Create task
                    task_data_wrapper = {
                        'task_id': task_idx,
                        'train_data': task_train_data,
                        'test_data': test_tensors[:len(task_train_data)] if len(test_tensors) > 0 else torch.tensor([]),
                        'train_indices': task_indices.tolist(),
                        'num_train_samples': len(task_indices),
                        'task_config': task_config_dict,
                        'difficulty_level': task_idx + 1
                    }
                    
                    gan_tasks.append(task_data_wrapper)
                    progressive_configs.append(task_config_dict)
                    
                    print(f'Created Difficulty Task {task_idx+1}: {len(task_indices)} samples')
        
        # Create task summary
        task_summary = {
            'total_tasks_created': len(gan_tasks),
            'model_type': model_type,
            'training_algorithm': training_algorithm,
            'task_type': task_type,
            'progressive_resolution': progressive_resolution,
            'incremental_complexity': incremental_complexity,
            'task_details': []
        }
        
        for task in gan_tasks:
            task_summary['task_details'].append({
                'task_id': task['task_id'],
                'train_samples': task['num_train_samples'],
                'test_samples': task.get('num_test_samples', 0),
                'description': task['task_config']['description']
            })
        
        # Save outputs
        os.makedirs(os.path.dirname(args.gan_tasks_path) or '.', exist_ok=True)
        with open(args.gan_tasks_path, 'wb') as f:
            pickle.dump(gan_tasks, f)
        
        os.makedirs(os.path.dirname(args.task_summary_path) or '.', exist_ok=True)
        with open(args.task_summary_path, 'w') as f:
            json.dump(task_summary, f, indent=2)
        
        os.makedirs(os.path.dirname(args.progressive_configs_path) or '.', exist_ok=True)
        with open(args.progressive_configs_path, 'w') as f:
            json.dump(progressive_configs, f, indent=2)
        
        print(f'Created {len(gan_tasks)} GAN continual learning tasks')
        print(f'Task Summary: {args.task_summary_path}')
        print(f'Progressive Configs: {args.progressive_configs_path}')
    args:
      - --train_data_path
      - {inputPath: train_data}
      - --test_data_path
      - {inputPath: test_data}
      - --model_config_str
      - {inputValue: model_config}
      - --task_config_str
      - {inputValue: task_config}
      - --gan_tasks_path
      - {outputPath: gan_tasks}
      - --task_summary_path
      - {outputPath: task_summary}
      - --progressive_configs_path
      - {outputPath: progressive_configs}
