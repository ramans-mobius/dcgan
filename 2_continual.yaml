name: Finetune GAN Continual Learning
description: Finetunes GAN model on continual learning tasks sequentially.
inputs:
  - name: base_model
    type: Model
  - name: cl_tasks
    type: Dataset
  - name: finetune_config
    type: String
    description: "Finetuning configuration"
outputs:
  - name: finetuned_models
    type: Dataset
    description: "List of finetuned models per task"
  - name: cl_history
    type: String
    description: "Continual learning history"
  - name: consolidated_model
    type: Model
    description: "Final consolidated model"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v33
    command:
      - sh
      - -c
      - |
        # Install dependencies
        pip install torch torchvision pillow numpy --quiet
        echo "Dependencies installed"
        
        python -c "
        import sys, os, pickle, json, copy, time
        import torch
        from torch.utils.data import DataLoader
        import numpy as np
        
        print('Starting GAN Continual Learning Finetuning')
        
        # Parse arguments
        base_model_path = sys.argv[1]
        cl_tasks_path = sys.argv[2]
        finetune_config_str = sys.argv[3]
        finetuned_models_path = sys.argv[4]
        cl_history_path = sys.argv[5]
        consolidated_model_path = sys.argv[6]
        
        # Load base model
        with open(base_model_path, 'rb') as f:
            base_model = pickle.load(f)
        
        # Load continual learning tasks
        with open(cl_tasks_path, 'rb') as f:
            cl_tasks = pickle.load(f)
        
        # Parse config
        try:
            finetune_config = json.loads(finetune_config_str)
        except:
            finetune_config = {}
        
        print(f'Starting continual learning with {len(cl_tasks)} tasks')
        
        # Training history
        cl_history = {
            'tasks': [],
            'model_type': getattr(base_model.config, 'model_type', 'dcgan') if hasattr(base_model, 'config') else 'unknown',
            'training_algo': getattr(base_model.config, 'training_algorithm', 'backprop') if hasattr(base_model, 'config') else 'backprop'
        }
        
        finetuned_models = []
        current_model = copy.deepcopy(base_model)
        
        # Continual learning loop
        for task_idx, task_data in enumerate(cl_tasks):
            task_start_time = time.time()
            
            task_id = task_data.get('task_id', task_idx)
            task_dataset = task_data.get('dataset')
            task_config = task_data.get('config', {})
            
            print(f'=== Task {task_id+1}/{len(cl_tasks)} ===')
            print(f'Description: {task_data.get(\"description\", \"No description\")}')
            print(f'Samples: {len(task_dataset)}')
            
            # Create dataloader for this task
            batch_size = task_config.get('batch_size', finetune_config.get('batch_size', 16))
            task_loader = DataLoader(
                task_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0
            )
            
            # Finetune model on this task
            try:
                # Adjust learning rate for fine-tuning
                original_lr = getattr(current_model.config, 'learning_rate', 0.0002) if hasattr(current_model, 'config') else 0.0002
                ft_lr = task_config.get('learning_rate', finetune_config.get('learning_rate', original_lr * 0.1))
                
                # If model has trainer, use it
                if hasattr(current_model, 'trainer'):
                    # Set lower learning rate for fine-tuning
                    if hasattr(current_model.trainer, 'gen_optimizer'):
                        for param_group in current_model.trainer.gen_optimizer.param_groups:
                            param_group['lr'] = ft_lr
                    if hasattr(current_model.trainer, 'disc_optimizer'):
                        for param_group in current_model.trainer.disc_optimizer.param_groups:
                            param_group['lr'] = ft_lr
                    
                    # Train for fewer epochs
                    ft_epochs = task_config.get('epochs', finetune_config.get('epochs_per_task', 10))
                    
                    task_metrics = {
                        'task_id': task_id,
                        'epochs': ft_epochs,
                        'learning_rate': ft_lr,
                        'samples': len(task_dataset)
                    }
                    
                    # Simple training loop for continual learning
                    for epoch in range(ft_epochs):
                        epoch_d_loss = 0
                        epoch_g_loss = 0
                        batch_count = 0
                        
                        for batch_data in task_loader:
                            real_imgs = batch_data
                            
                            # Train discriminator
                            if hasattr(current_model.trainer, 'train_discriminator'):
                                d_metrics = current_model.trainer.train_discriminator(real_imgs)
                                epoch_d_loss += d_metrics.get('d_loss', 0)
                            
                            # Train generator
                            if hasattr(current_model.trainer, 'train_generator'):
                                g_metrics = current_model.trainer.train_generator(real_imgs.size(0))
                                epoch_g_loss += g_metrics.get('g_loss', 0)
                            
                            batch_count += 1
                        
                        avg_d_loss = epoch_d_loss / batch_count if batch_count > 0 else 0
                        avg_g_loss = epoch_g_loss / batch_count if batch_count > 0 else 0
                        
                        if epoch % max(1, ft_epochs // 5) == 0:
                            print(f'  Epoch {epoch+1}/{ft_epochs}: D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}')
                    
                    task_metrics['final_d_loss'] = avg_d_loss
                    task_metrics['final_g_loss'] = avg_g_loss
                    
                else:
                    print('  Model has no trainer, skipping fine-tuning for this task')
                    task_metrics = {
                        'task_id': task_id,
                        'skipped': True,
                        'reason': 'No trainer available'
                    }
                
            except Exception as e:
                print(f'  Error fine-tuning task {task_id}: {e}')
                task_metrics = {
                    'task_id': task_id,
                    'error': str(e),
                    'skipped': True
                }
            
            task_time = time.time() - task_start_time
            task_metrics['training_time'] = task_time
            
            # Save task model
            task_model_copy = copy.deepcopy(current_model)
            finetuned_models.append({
                'task_id': task_id,
                'model': task_model_copy,
                'metrics': task_metrics,
                'config': task_config
            })
            
            cl_history['tasks'].append(task_metrics)
            
            print(f'Task {task_id+1} completed in {task_time:.1f}s')
        
        # Create consolidated model (average weights or last model)
        consolidated_model = copy.deepcopy(current_model)
        
        # Save outputs
        os.makedirs(os.path.dirname(finetuned_models_path) or '.', exist_ok=True)
        with open(finetuned_models_path, 'wb') as f:
            pickle.dump(finetuned_models, f)
        
        os.makedirs(os.path.dirname(cl_history_path) or '.', exist_ok=True)
        with open(cl_history_path, 'w') as f:
            json.dump(cl_history, f, indent=2)
        
        os.makedirs(os.path.dirname(consolidated_model_path) or '.', exist_ok=True)
        with open(consolidated_model_path, 'wb') as f:
            pickle.dump(consolidated_model, f)
        
        print(f'Continual Learning Complete!')
        print(f'  Tasks processed: {len(cl_tasks)}')
        print(f'  Models saved: {len(finetuned_models)}')
        print(f'  Final model saved to: {consolidated_model_path}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputPath: base_model}
      - {inputPath: cl_tasks}
      - {inputValue: finetune_config}
      - {outputPath: finetuned_models}
      - {outputPath: cl_history}
      - {outputPath: consolidated_model}
