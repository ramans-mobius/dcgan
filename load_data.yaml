name: 2 DCGAN Load Data
description: Download ZIP from CDN, extract UI component images, preserve original structure
inputs:
  - {name: cdn_url, type: String, description: "CDN URL to download ZIP file"}
  - {name: train_split, type: Float, optional: true, default: "0.7", description: "Train split ratio (0.0-1.0)"}
  - {name: shuffle_seed, type: Integer, optional: true, default: "42", description: "Random seed for shuffling"}
  - {name: target_column, type: String, optional: true, default: "label", description: "Name for target column in output"}
  - {name: max_samples_per_class, type: Integer, optional: true, default: "0", description: "Maximum samples per class (0 for all)"}
outputs:
  - {name: original_dataset, type: Dataset, description: "Pickle file with DataWrapper preserving original structure"}
  - {name: train_split_info, type: Data, description: "Train/Test split indices"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v32
    command:
      - sh
      - -c
      - |
        python3 -c "
        import sys, json, os, pickle, zipfile, io, requests, numpy as np, random, pandas as pd, base64, tempfile, hashlib, pathlib, datetime, traceback
        from urllib.parse import unquote
        from PIL import Image
        from collections import Counter, defaultdict
        
        print('Number of arguments:', len(sys.argv))
        for i, arg in enumerate(sys.argv):
            print(f'  Argument {i}: {arg[:100] if len(str(arg)) > 100 else arg}')
        
        # Get args - match working pattern
        if len(sys.argv) < 8:
            raise ValueError(f'Expected 8 arguments, got {len(sys.argv)}')
        
        cdn_url = sys.argv[1]                # $0
        train_split = float(sys.argv[2])     # $1
        shuffle_seed = int(sys.argv[3])      # $2
        target_column = sys.argv[4]          # $3
        max_samples_per_class = int(sys.argv[5])  # $4
        original_dataset_path = sys.argv[6]  # $5
        train_split_info_path = sys.argv[7]  # $6
        
        print('='*80)
        print('DCGAN LOAD DATA')
        print('='*80)
        
        class DataWrapper:
            def __init__(self):
                self.images = []
                self.class_names = []
                self.class_to_idx = {}
                self.idx_to_class = {}
                self.metadata = {}
                self.tabular_data = None
            
            def add_image(self, image_data, label, file_path, class_name):
                self.images.append({
                    'image_data': image_data,
                    'label': label,
                    'file_path': file_path,
                    'class_name': class_name,
                    'image_id': hashlib.md5(image_data).hexdigest()[:16]
                })
            
            def build_mappings(self):
                unique_classes = sorted(set([img['class_name'] for img in self.images]))
                self.class_names = unique_classes
                self.class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}
                self.idx_to_class = {idx: cls for idx, cls in enumerate(unique_classes)}
                for img in self.images:
                    img['label_idx'] = self.class_to_idx[img['class_name']]
            
            def get_class_distribution(self):
                distribution = Counter([img['class_name'] for img in self.images])
                return dict(distribution)
        
        def download_and_extract(url, temp_dir):
            print(f'[INFO] Downloading from: {url}')
            decoded_url = requests.utils.unquote(url)
            response = requests.get(decoded_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=60)
            response.raise_for_status()
            
            zip_path = os.path.join(temp_dir, 'dataset.zip')
            with open(zip_path, 'wb') as f:
                f.write(response.content)
            
            print(f'[INFO] Downloaded {len(response.content)} bytes')
            
            data_wrapper = DataWrapper()
            class_folders = set()
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                all_files = zip_ref.namelist()
                for file_path in all_files:
                    if not file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                        continue
                    
                    parts = pathlib.Path(file_path).parts
                    if len(parts) < 3:
                        print(f'[WARN] Skipping file at unexpected depth: {file_path}')
                        continue
                    
                    class_name = parts[-2]
                    if class_name.startswith('.') or class_name.startswith('_'):
                        continue
                    
                    with zip_ref.open(file_path) as img_file:
                        image_data = img_file.read()
                        data_wrapper.add_image(image_data, class_name, file_path, class_name)
                        class_folders.add(class_name)
            
            data_wrapper.build_mappings()
            print(f'[INFO] Found {len(data_wrapper.images)} images')
            print(f'[INFO] Classes: {sorted(class_folders)}')
            return data_wrapper
        
        def create_split_indices(data_wrapper, train_split_val, shuffle_seed_val):
            print(f'[INFO] Creating stratified split with ratio: {train_split_val}')
            class_groups = defaultdict(list)
            for idx, img in enumerate(data_wrapper.images):
                class_groups[img['class_name']].append(idx)
            
            train_indices = []
            test_indices = []
            random.seed(shuffle_seed_val)
            np.random.seed(shuffle_seed_val)
            
            for class_name, indices in class_groups.items():
                n_total = len(indices)
                n_train = int(n_total * train_split_val)
                shuffled = indices.copy()
                random.shuffle(shuffled)
                train_indices.extend(shuffled[:n_train])
                test_indices.extend(shuffled[n_train:])
            
            random.shuffle(train_indices)
            random.shuffle(test_indices)
            
            print(f'[INFO] Train samples: {len(train_indices)}')
            print(f'[INFO] Test samples: {len(test_indices)}')
            return np.array(train_indices), np.array(test_indices)
        
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                # 1. Download and extract
                data_wrapper = download_and_extract(cdn_url, temp_dir)
                
                # 2. Apply max samples limit
                if max_samples_per_class > 0:
                    print(f'[INFO] Limiting to {max_samples_per_class} samples per class')
                    class_groups = defaultdict(list)
                    for img in data_wrapper.images:
                        class_groups[img['class_name']].append(img)
                    
                    limited_images = []
                    for class_name, images in class_groups.items():
                        limited_images.extend(images[:max_samples_per_class])
                    
                    new_wrapper = DataWrapper()
                    for img in limited_images:
                        new_wrapper.add_image(img['image_data'], img['label'], img['file_path'], img['class_name'])
                    new_wrapper.build_mappings()
                    data_wrapper = new_wrapper
                
                # 3. Create split indices
                train_indices, test_indices = create_split_indices(data_wrapper, train_split, shuffle_seed)
                
                # 4. Save outputs
                os.makedirs(os.path.dirname(original_dataset_path) or '.', exist_ok=True)
                os.makedirs(os.path.dirname(train_split_info_path) or '.', exist_ok=True)
                
                with open(original_dataset_path, 'wb') as f:
                    pickle.dump(data_wrapper, f)
                print(f'[INFO] Saved original dataset to: {original_dataset_path}')
                
                split_info = {
                    'train_indices': train_indices.tolist(),
                    'test_indices': test_indices.tolist(),
                    'train_size': len(train_indices),
                    'test_size': len(test_indices),
                    'class_names': data_wrapper.class_names,
                    'timestamp': datetime.datetime.utcnow().isoformat() + 'Z'
                }
                
                with open(train_split_info_path, 'w') as f:
                    json.dump(split_info, f, indent=2)
                print(f'[INFO] Saved split info to: {train_split_info_path}')
                
                print('='*80)
                print('LOAD PROCESS COMPLETE')
                print('='*80)
                print(f'Total images: {len(data_wrapper.images)}')
                print(f'Classes: {data_wrapper.class_names}')
                print(f'Train samples: {len(train_indices)}')
                print(f'Test samples: {len(test_indices)}')
                
        except Exception as e:
            print(f'[ERROR] Failed: {e}')
            traceback.print_exc()
            sys.exit(1)
        
        print('Data loading completed successfully!')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputValue: cdn_url}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {inputValue: target_column}
      - {inputValue: max_samples_per_class}
      - {outputPath: original_dataset}
      - {outputPath: train_split_info}
