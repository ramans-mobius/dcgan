name: Evaluate GAN Model Comprehensive
description: Comprehensive GAN evaluation with FID, precision, recall, F1, and all metrics.
inputs:
  - name: trained_model
    type: Model
  - name: test_data
    type: Dataset
  - name: training_history
    type: String
  - name: eval_config
    type: String
    description: "Evaluation configuration"
outputs:
  - name: eval_results
    type: String
    description: "Complete evaluation metrics JSON"
  - name: final_images
    type: Dataset
    description: "Generated images for analysis"
  - name: metrics_summary
    type: String
    description: "Text summary of all metrics"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v33
    command:
      - sh
      - -c
      - |
        # Install comprehensive dependencies
        pip install torch torchvision pillow numpy scipy scikit-learn --quiet
        echo "Dependencies installed"
        
        python -c "
        import sys, os, pickle, json, base64, io, math, time, warnings
        import numpy as np
        import torch
        from torch.utils.data import DataLoader
        from PIL import Image
        from scipy import linalg
        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
        from sklearn.neighbors import NearestNeighbors
        warnings.filterwarnings('ignore')
        
        print('Starting Comprehensive GAN Evaluation')
        
        # Parse arguments
        trained_model_path = sys.argv[1]
        test_data_path = sys.argv[2]
        training_history_str = sys.argv[3]
        eval_config_str = sys.argv[4]
        eval_results_path = sys.argv[5]
        final_images_path = sys.argv[6]
        metrics_summary_path = sys.argv[7]
        
        print('Loading model and data...')
        
        # Load trained model
        with open(trained_model_path, 'rb') as f:
            gan_model = pickle.load(f)
        
        # Load test data
        with open(test_data_path, 'rb') as f:
            test_data_wrapper = pickle.load(f)
        
        # Parse configs
        try:
            training_history = json.loads(training_history_str)
        except:
            training_history = {}
        
        try:
            eval_config = json.loads(eval_config_str)
        except:
            eval_config = {}
        
        # Determine model type
        model_type = 'dcgan'
        if hasattr(gan_model, 'config'):
            if hasattr(gan_model.config, 'model_type'):
                model_type = gan_model.config.model_type
            elif hasattr(gan_model.config, 'input_dim'):
                model_type = 'vanilla_gan'
        
        print(f'Evaluating {model_type.upper()} GAN with comprehensive metrics')
        
        # Get dataset from wrapper
        test_dataset = test_data_wrapper.get('dataset')
        if not test_dataset:
            print('No test dataset found')
            test_dataset = []
        
        # Extract model info
        model_info = {
            'model_type': model_type,
            'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }
        
        # Add model-specific info
        if hasattr(gan_model, 'config'):
            config_dict = gan_model.config.__dict__ if hasattr(gan_model.config, '__dict__') else gan_model.config
            for key in ['latent_dim', 'image_size', 'channels', 'input_dim', 'training_algorithm', 'generator_layers', 'discriminator_layers']:
                if key in config_dict:
                    model_info[key] = config_dict[key]
        
        # Initialize metrics dictionary
        metrics = {
            'model_info': model_info,
            'training_metrics': {},
            'generation_metrics': {},
            'quality_metrics': {},
            'diversity_metrics': {},
            'classification_metrics': {}
        }
        
        # 1. Extract training metrics from history
        if training_history and isinstance(training_history, dict):
            print('Analyzing training history...')
            
            training_metrics = {}
            for key in ['d_loss', 'g_loss', 'real_score', 'fake_score']:
                if key in training_history and training_history[key]:
                    values = training_history[key]
                    if values:
                        training_metrics[f'final_{key}'] = float(values[-1])
                        training_metrics[f'min_{key}'] = float(min(values))
                        training_metrics[f'max_{key}'] = float(max(values))
                        training_metrics[f'avg_{key}'] = float(sum(values)/len(values))
                        training_metrics[f'std_{key}'] = float(np.std(values))
            
            # Calculate training stability
            if 'd_loss' in training_history and training_history['d_loss']:
                d_losses = training_history['d_loss']
                if len(d_losses) > 10:
                    n = len(d_losses)
                    first_10 = d_losses[:n//10]
                    last_10 = d_losses[-n//10:]
                    
                    training_metrics['loss_convergence'] = float(abs(np.mean(last_10) - np.mean(first_10)))
                    training_metrics['loss_stability'] = float(np.std(d_losses[-n//4:]))
                    training_metrics['training_progress'] = float((d_losses[0] - d_losses[-1]) / d_losses[0] if d_losses[0] > 0 else 0)
            
            metrics['training_metrics'] = training_metrics
        
        # 2. Generate samples for evaluation
        sample_count = eval_config.get('num_samples', 100)
        final_images = []
        generated_samples = []
        
        try:
            if hasattr(gan_model, 'generator') and gan_model.generator:
                print(f'Generating {sample_count} samples for evaluation...')
                
                # Get latent dimension
                latent_dim = model_info.get('latent_dim', 100)
                image_size = model_info.get('image_size', 64)
                channels = model_info.get('channels', 3)
                
                # Generate noise
                gan_model.generator.eval()
                with torch.no_grad():
                    # Generate multiple batches
                    batch_size = min(32, sample_count)
                    for i in range(0, sample_count, batch_size):
                        current_batch = min(batch_size, sample_count - i)
                        noise = torch.randn(current_batch, latent_dim)
                        
                        # Add spatial dimensions for DCGAN
                        if model_type == 'dcgan':
                            noise = noise.view(current_batch, latent_dim, 1, 1)
                        
                        # Generate images
                        samples = gan_model.generator(noise).cpu()
                        
                        # Denormalize from [-1, 1] to [0, 1]
                        samples = (samples + 1) / 2
                        samples = torch.clamp(samples, 0, 1)
                        
                        # Store for metrics calculation
                        generated_samples.append(samples.numpy())
                        
                        # Convert to base64 for storage
                        for j in range(samples.size(0)):
                            sample = samples[j]
                            
                            # Determine image dimensions
                            if len(sample.shape) == 3:
                                # DCGAN format [C, H, W]
                                if sample.shape[0] == 1:
                                    img_np = sample.squeeze(0).numpy()
                                    img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='L')
                                else:
                                    img_np = sample.permute(1, 2, 0).numpy()
                                    img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='RGB')
                            else:
                                img_size = int(math.sqrt(sample.shape[0]))
                                img_np = sample.view(img_size, img_size).numpy()
                                img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='L')
                            
                            # Convert to base64
                            img_bytes = io.BytesIO()
                            img_pil.save(img_bytes, format='PNG')
                            base64_data = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
                            
                            # Calculate image statistics
                            img_mean = float(np.mean(img_np))
                            img_std = float(np.std(img_np))
                            
                            final_images.append({
                                'sample_id': i + j,
                                'image_data': base64_data,
                                'model_type': model_type,
                                'filename': f'{model_type}_sample_{i+j}.png',
                                'brightness': img_mean,
                                'contrast': img_std,
                                'metrics_ready': True
                            })
                
                # Combine all generated samples
                if generated_samples:
                    generated_samples = np.concatenate(generated_samples, axis=0)
                    print(f'Generated {generated_samples.shape[0]} samples for metrics calculation')
                
        except Exception as e:
            print(f'Error generating samples: {e}')
        
        metrics['generation_metrics']['samples_generated'] = len(final_images)
        metrics['generation_metrics']['generation_success_rate'] = len(final_images) / sample_count if sample_count > 0 else 0
        
        # 3. Calculate Image Quality Metrics
        print('Calculating image quality metrics...')
        
        if len(generated_samples) > 0:
            # Flatten images for some metrics
            if model_type == 'dcgan' and len(generated_samples.shape) == 4:
                batch_size, c, h, w = generated_samples.shape
                flattened_generated = generated_samples.reshape(batch_size, -1)
            else:
                flattened_generated = generated_samples
            
            # Pixel statistics
            metrics['quality_metrics']['pixel_mean'] = float(np.mean(generated_samples))
            metrics['quality_metrics']['pixel_std'] = float(np.std(generated_samples))
            metrics['quality_metrics']['pixel_min'] = float(np.min(generated_samples))
            metrics['quality_metrics']['pixel_max'] = float(np.max(generated_samples))
            
            # Contrast
            contrast_scores = []
            for img in generated_samples[:min(50, len(generated_samples))]:
                if len(img.shape) == 3:
                    img_gray = np.mean(img, axis=0) if img.shape[0] == 3 else img[0]
                else:
                    img_gray = img
                contrast_scores.append(np.std(img_gray))
            metrics['quality_metrics']['contrast_mean'] = float(np.mean(contrast_scores))
            metrics['quality_metrics']['contrast_std'] = float(np.std(contrast_scores))
        
        # 4. Calculate Diversity Metrics
        print('Calculating diversity metrics...')
        
        if len(generated_samples) > 10:
            # Flatten for distance calculations
            if model_type == 'dcgan' and len(generated_samples.shape) == 4:
                batch_size, c, h, w = generated_samples.shape
                flattened = generated_samples.reshape(batch_size, -1)
            else:
                flattened = generated_samples
            
            # Intra-class distance
            try:
                from scipy.spatial.distance import pdist
                subset_size = min(100, len(flattened))
                indices = np.random.choice(len(flattened), subset_size, replace=False)
                subset = flattened[indices]
                
                distances = pdist(subset, metric='euclidean')
                metrics['diversity_metrics']['intra_distance_mean'] = float(np.mean(distances))
                metrics['diversity_metrics']['intra_distance_std'] = float(np.std(distances))
                metrics['diversity_metrics']['intra_distance_min'] = float(np.min(distances))
                metrics['diversity_metrics']['intra_distance_max'] = float(np.max(distances))
            except:
                pass
            
            # Nearest neighbors for coverage
            try:
                nbrs = NearestNeighbors(n_neighbors=min(5, len(flattened)-1), metric='euclidean')
                nbrs.fit(flattened)
                distances, _ = nbrs.kneighbors(flattened)
                avg_nearest_dist = np.mean(distances[:, 1:])
                metrics['diversity_metrics']['avg_nearest_neighbor_dist'] = float(avg_nearest_dist)
                
                coverage = min(1.0, avg_nearest_dist / (np.std(flattened) + 1e-8))
                metrics['diversity_metrics']['coverage_estimate'] = float(coverage)
            except:
                pass
        
        # 5. Calculate FID-like metric
        print('Calculating distribution metrics...')
        
        if len(generated_samples) > 50 and test_dataset and len(test_dataset) > 50:
            try:
                # Get real samples from test dataset
                real_samples = []
                batch_size = min(32, len(test_dataset))
                real_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)
                
                for batch in real_loader:
                    real_samples.append(batch.numpy())
                    if len(real_samples) * batch_size >= 100:
                        break
                
                if real_samples:
                    real_samples = np.concatenate(real_samples, axis=0)
                    
                    # Ensure same shape
                    if model_type == 'dcgan' and len(real_samples.shape) == 2:
                        img_size = int(math.sqrt(real_samples.shape[1]))
                        real_samples = real_samples.reshape(-1, 1, img_size, img_size)
                    
                    # Flatten for distribution comparison
                    if model_type == 'dcgan' and len(real_samples.shape) == 4:
                        real_flat = real_samples.reshape(real_samples.shape[0], -1)
                        gen_flat = generated_samples[:real_flat.shape[0]].reshape(real_flat.shape[0], -1)
                    else:
                        real_flat = real_samples
                        gen_flat = generated_samples[:real_flat.shape[0]]
                    
                    # Calculate simplified FID
                    mu_real = np.mean(real_flat, axis=0)
                    mu_gen = np.mean(gen_flat, axis=0)
                    sigma_real = np.cov(real_flat, rowvar=False)
                    sigma_gen = np.cov(gen_flat, rowvar=False)
                    
                    diff = mu_real - mu_gen
                    fid = np.dot(diff, diff) + np.trace(sigma_real + sigma_gen - 2 * np.sqrt(sigma_real @ sigma_gen))
                    metrics['quality_metrics']['simplified_fid'] = float(fid)
                    
                    # Frechet Distance
                    covmean = linalg.sqrtm(sigma_real.dot(sigma_gen))
                    if np.iscomplexobj(covmean):
                        covmean = covmean.real
                    
                    frechet = np.sum((mu_real - mu_gen)**2) + np.trace(sigma_real + sigma_gen - 2*covmean)
                    metrics['quality_metrics']['frechet_distance'] = float(frechet)
                    
            except Exception as e:
                print(f'Error calculating distribution metrics: {e}')
        
        # 6. Calculate Classification Metrics (F1, Precision, Recall)
        print('Calculating classification metrics...')
        
        if eval_config.get('calculate_classification_metrics', False) and hasattr(gan_model, 'discriminator'):
            try:
                gan_model.discriminator.eval()
                with torch.no_grad():
                    # Get real data predictions
                    real_preds = []
                    real_labels = []
                    real_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)
                    
                    for batch in real_loader:
                        if isinstance(batch, torch.Tensor):
                            real_data = batch
                        else:
                            real_data = batch[0] if isinstance(batch, (list, tuple)) else batch
                        
                        preds = gan_model.discriminator(real_data)
                        real_preds.extend(preds.numpy())
                        real_labels.extend([1] * len(preds))
                    
                    # Get fake data predictions
                    fake_preds = []
                    fake_labels = []
                    noise = torch.randn(len(real_preds), latent_dim)
                    if model_type == 'dcgan':
                        noise = noise.view(-1, latent_dim, 1, 1)
                    
                    fake_data = gan_model.generator(noise)
                    fake_preds = gan_model.discriminator(fake_data).numpy()
                    fake_labels = [0] * len(fake_preds)
                    
                    # Combine predictions
                    all_preds = np.concatenate([real_preds, fake_preds])
                    all_labels = np.concatenate([real_labels, fake_labels])
                    
                    # Convert to binary predictions
                    binary_preds = (all_preds > 0.5).astype(int)
                    
                    # Calculate all classification metrics
                    metrics['classification_metrics']['accuracy'] = float(accuracy_score(all_labels, binary_preds))
                    metrics['classification_metrics']['precision'] = float(precision_score(all_labels, binary_preds, zero_division=0))
                    metrics['classification_metrics']['recall'] = float(recall_score(all_labels, binary_preds, zero_division=0))
                    metrics['classification_metrics']['f1_score'] = float(f1_score(all_labels, binary_preds, zero_division=0))
                    metrics['classification_metrics']['discriminator_real_confidence'] = float(np.mean(real_preds))
                    metrics['classification_metrics']['discriminator_fake_confidence'] = float(np.mean(fake_preds))
                    
            except Exception as e:
                print(f'Error calculating classification metrics: {e}')
        
        # 7. Calculate Overall Scores
        print('Calculating overall scores...')
        
        # Quality Score (0-100)
        quality_score = 0
        if 'contrast_mean' in metrics['quality_metrics']:
            quality_score += min(40, metrics['quality_metrics']['contrast_mean'] * 100)
        if 'simplified_fid' in metrics['quality_metrics']:
            fid_score = metrics['quality_metrics']['simplified_fid']
            quality_score += min(30, max(0, 100 - fid_score))
        
        # Diversity Score (0-100)
        diversity_score = 0
        if 'intra_distance_mean' in metrics['diversity_metrics']:
            diversity_score = min(100, metrics['diversity_metrics']['intra_distance_mean'] * 10)
        
        # Training Score (0-100)
        training_score = 0
        if 'final_d_loss' in metrics['training_metrics']:
            d_loss = metrics['training_metrics']['final_d_loss']
            g_loss = metrics['training_metrics'].get('final_g_loss', d_loss)
            training_score = min(100, max(0, 100 - (d_loss + g_loss) * 50))
        
        # Classification Score (0-100) - if available
        classification_score = 0
        if 'f1_score' in metrics['classification_metrics']:
            classification_score = metrics['classification_metrics']['f1_score'] * 100
        
        # Overall Score
        if classification_score > 0:
            overall_score = (
                quality_score * 0.25 +
                diversity_score * 0.25 +
                training_score * 0.25 +
                classification_score * 0.25
            )
        else:
            overall_score = (
                quality_score * 0.35 +
                diversity_score * 0.35 +
                training_score * 0.30
            )
        
        metrics['overall_scores'] = {
            'quality_score': quality_score,
            'diversity_score': diversity_score,
            'training_score': training_score,
            'classification_score': classification_score,
            'overall_score': overall_score,
            'rating': 'EXCELLENT' if overall_score >= 80 else 'GOOD' if overall_score >= 60 else 'FAIR' if overall_score >= 40 else 'POOR'
        }
        
        # 8. Create comprehensive text summary
        summary = '=' * 70 + '\\n'
        summary += f'{model_type.upper()} GAN COMPREHENSIVE EVALUATION REPORT\\n'
        summary += '=' * 70 + '\\n\\n'
        
        summary += f'Generated: {model_info[\"evaluation_time\"]}\\n'
        summary += f'Model Type: {model_type}\\n'
        summary += f'Training Algorithm: {model_info.get(\"training_algorithm\", \"N/A\")}\\n'
        summary += f'Image Size: {model_info.get(\"image_size\", \"N/A\")}\\n'
        summary += f'Latent Dimension: {model_info.get(\"latent_dim\", \"N/A\")}\\n\\n'
        
        summary += 'TRAINING METRICS:\\n'
        summary += f'  Final D Loss: {metrics[\"training_metrics\"].get(\"final_d_loss\", \"N/A\")}\\n'
        summary += f'  Final G Loss: {metrics[\"training_metrics\"].get(\"final_g_loss\", \"N/A\")}\\n'
        summary += f'  Loss Convergence: {metrics[\"training_metrics\"].get(\"loss_convergence\", \"N/A\")}\\n\\n'
        
        summary += 'GENERATION METRICS:\\n'
        summary += f'  Samples Generated: {metrics[\"generation_metrics\"][\"samples_generated\"]}\\n'
        summary += f'  Success Rate: {metrics[\"generation_metrics\"][\"generation_success_rate\"]:.1%}\\n\\n'
        
        summary += 'QUALITY METRICS:\\n'
        summary += f'  Pixel Mean: {metrics[\"quality_metrics\"].get(\"pixel_mean\", \"N/A\"):.4f}\\n'
        summary += f'  Contrast: {metrics[\"quality_metrics\"].get(\"contrast_mean\", \"N/A\"):.4f}\\n'
        summary += f'  Simplified FID: {metrics[\"quality_metrics\"].get(\"simplified_fid\", \"N/A\"):.2f}\\n\\n'
        
        summary += 'DIVERSITY METRICS:\\n'
        summary += f'  Intra-sample Distance: {metrics[\"diversity_metrics\"].get(\"intra_distance_mean\", \"N/A\"):.4f}\\n'
        summary += f'  Coverage Estimate: {metrics[\"diversity_metrics\"].get(\"coverage_estimate\", \"N/A\"):.1%}\\n\\n'
        
        if metrics['classification_metrics']:
            summary += 'CLASSIFICATION METRICS:\\n'
            summary += f'  Accuracy: {metrics[\"classification_metrics\"].get(\"accuracy\", \"N/A\"):.3f}\\n'
            summary += f'  Precision: {metrics[\"classification_metrics\"].get(\"precision\", \"N/A\"):.3f}\\n'
            summary += f'  Recall: {metrics[\"classification_metrics\"].get(\"recall\", \"N/A\"):.3f}\\n'
            summary += f'  F1 Score: {metrics[\"classification_metrics\"].get(\"f1_score\", \"N/A\"):.3f}\\n\\n'
        
        summary += 'OVERALL SCORES:\\n'
        summary += f'  Quality Score: {quality_score:.1f}/100\\n'
        summary += f'  Diversity Score: {diversity_score:.1f}/100\\n'
        summary += f'  Training Score: {training_score:.1f}/100\\n'
        if classification_score > 0:
            summary += f'  Classification Score: {classification_score:.1f}/100\\n'
        summary += f'  OVERALL SCORE: {overall_score:.1f}/100 - {metrics[\"overall_scores\"][\"rating\"]}\\n\\n'
        
        summary += 'RECOMMENDATIONS:\\n'
        if overall_score >= 80:
            summary += '  ✓ Model is excellent and ready for deployment\\n'
        elif overall_score >= 60:
            summary += '  ⚠ Model is good but could be improved\\n'
        else:
            summary += '  ✗ Model needs significant improvement\\n'
        
        summary += '\\n' + '=' * 70
        
        print(summary)
        
        # 9. Save all outputs
        print('Saving evaluation results...')
        
        os.makedirs(os.path.dirname(eval_results_path) or '.', exist_ok=True)
        with open(eval_results_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        with open(final_images_path, 'wb') as f:
            pickle.dump(final_images, f)
        
        with open(metrics_summary_path, 'w') as f:
            f.write(summary)
        
        print('Comprehensive GAN Evaluation Complete!')
        print(f'Results saved to: {eval_results_path}')
        print(f'Generated {len(final_images)} images for CDN upload')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputPath: trained_model}
      - {inputPath: test_data}
      - {inputPath: training_history}
      - {inputValue: eval_config}
      - {outputPath: eval_results}
      - {outputPath: final_images}
      - {outputPath: metrics_summary}
