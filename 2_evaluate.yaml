name: Evaluate DCGAN Model
description: Evaluates DCGAN model and generates final images.
inputs:
  - name: trained_model
    type: Model
  - name: test_data
    type: Dataset
  - name: config
    type: String
    description: "Evaluation configuration"
outputs:
  - name: eval_metrics
    type: String
    description: "Evaluation metrics JSON"
  - name: generated_images
    type: Dataset
    description: "Final generated images"
  - name: evaluation_report
    type: String
    description: "Human-readable evaluation report"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v33
    command:
      - sh
      - -c
      - |
        python -c "
        import sys, os, pickle, json, base64, io
        import torch
        import numpy as np
        from PIL import Image
        
        print('Starting DCGAN Model Evaluation')
        
        # Parse arguments
        trained_model_path = sys.argv[1]
        test_data_path = sys.argv[2]
        config_str = sys.argv[3]
        eval_metrics_path = sys.argv[4]
        generated_images_path = sys.argv[5]
        evaluation_report_path = sys.argv[6]
        
        print('Loading model and data...')
        
        # Load trained model
        with open(trained_model_path, 'rb') as f:
            dcgan_model = pickle.load(f)
        
        # Load test data (if needed for comparison)
        test_data = None
        if os.path.exists(test_data_path):
            with open(test_data_path, 'rb') as f:
                test_data = pickle.load(f)
        
        # Parse config
        try:
            config = json.loads(config_str)
        except:
            config = {}
        
        num_generate = config.get('num_generate', 16)
        image_size = getattr(dcgan_model.config, 'image_size', 64)
        
        print('Evaluation Parameters:')
        print('  Images to generate: ' + str(num_generate))
        print('  Image size: ' + str(image_size))
        
        # Generate final images
        generated_images = []
        metrics = {
            'generation_success': True,
            'num_images_generated': 0,
            'image_size': image_size,
            'model_parameters': {},
            'quality_metrics': {}
        }
        
        try:
            # Set model to eval mode
            dcgan_model.generator.eval()
            
            # Generate images
            with torch.no_grad():
                z = torch.randn(num_generate, dcgan_model.config.latent_dim, 1, 1)
                if hasattr(dcgan_model.config, 'device') and dcgan_model.config.device != 'cpu':
                    z = z.to(dcgan_model.config.device)
                
                samples = dcgan_model.generator(z)
                samples = samples.cpu()
            
            # Convert to base64 and collect
            for i in range(samples.size(0)):
                sample_img = samples[i]
                
                # Denormalize from [-1, 1] to [0, 1]
                if sample_img.min() < 0:
                    sample_img = (sample_img + 1) / 2
                
                # Ensure values are in [0, 1]
                sample_img = torch.clamp(sample_img, 0, 1)
                
                # Convert to PIL Image
                if sample_img.shape[0] == 1:  # Grayscale
                    img_np = sample_img.squeeze(0).numpy()
                    img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='L')
                else:  # RGB
                    img_np = sample_img.permute(1, 2, 0).numpy()
                    img_pil = Image.fromarray((img_np * 255).astype(np.uint8), mode='RGB')
                
                # Convert to base64
                img_bytes = io.BytesIO()
                img_pil.save(img_bytes, format='PNG')
                base64_data = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
                
                # Calculate some basic metrics
                img_array = np.array(img_pil)
                brightness = np.mean(img_array) / 255.0
                contrast = np.std(img_array) / 255.0
                
                generated_images.append({
                    'image_index': i,
                    'image_data': base64_data,
                    'brightness': float(brightness),
                    'contrast': float(contrast),
                    'filename': f'generated_{i:03d}.png'
                })
            
            metrics['num_images_generated'] = len(generated_images)
            
            # Calculate additional metrics
            if len(generated_images) > 1:
                brightness_values = [img['brightness'] for img in generated_images]
                contrast_values = [img['contrast'] for img in generated_images]
                
                metrics['quality_metrics'] = {
                    'avg_brightness': float(np.mean(brightness_values)),
                    'std_brightness': float(np.std(brightness_values)),
                    'avg_contrast': float(np.mean(contrast_values)),
                    'std_contrast': float(np.std(contrast_values)),
                    'diversity_score': float(np.std(brightness_values) + np.std(contrast_values))
                }
            
            print('Successfully generated ' + str(len(generated_images)) + ' images')
            
        except Exception as e:
            print('Error during image generation: ' + str(e))
            metrics['generation_success'] = False
            metrics['error'] = str(e)
        
        # Get model information
        try:
            metrics['model_parameters'] = {
                'latent_dim': getattr(dcgan_model.config, 'latent_dim', 100),
                'image_size': getattr(dcgan_model.config, 'image_size', 64),
                'channels': getattr(dcgan_model.config, 'channels', 3),
                'training_algorithm': getattr(dcgan_model.config, 'training_algorithm', 'backprop'),
                'generator_params': sum(p.numel() for p in dcgan_model.generator.parameters()),
                'discriminator_params': sum(p.numel() for p in dcgan_model.discriminator.parameters())
            }
        except:
            metrics['model_parameters'] = {'error': 'Could not extract model parameters'}
        
        # Create evaluation report
        report_lines = []
        report_lines.append('DCGAN Evaluation Report')
        report_lines.append('=' * 50)
        report_lines.append('Model Information:')
        report_lines.append(f"  Latent Dimension: {metrics['model_parameters'].get('latent_dim', 'N/A')}")
        report_lines.append(f"  Image Size: {metrics['model_parameters'].get('image_size', 'N/A')}")
        report_lines.append(f"  Channels: {metrics['model_parameters'].get('channels', 'N/A')}")
        report_lines.append(f"  Training Algorithm: {metrics['model_parameters'].get('training_algorithm', 'N/A')}")
        report_lines.append(f"  Generator Parameters: {metrics['model_parameters'].get('generator_params', 'N/A'):,}")
        report_lines.append(f"  Discriminator Parameters: {metrics['model_parameters'].get('discriminator_params', 'N/A'):,}")
        
        report_lines.append('\\nGeneration Results:')
        report_lines.append(f"  Success: {metrics['generation_success']}")
        report_lines.append(f"  Images Generated: {metrics['num_images_generated']}")
        
        if metrics['quality_metrics']:
            report_lines.append('\\nQuality Metrics:')
            report_lines.append(f"  Average Brightness: {metrics['quality_metrics'].get('avg_brightness', 0):.3f}")
            report_lines.append(f"  Brightness Std: {metrics['quality_metrics'].get('std_brightness', 0):.3f}")
            report_lines.append(f"  Average Contrast: {metrics['quality_metrics'].get('avg_contrast', 0):.3f}")
            report_lines.append(f"  Contrast Std: {metrics['quality_metrics'].get('std_contrast', 0):.3f}")
            report_lines.append(f"  Diversity Score: {metrics['quality_metrics'].get('diversity_score', 0):.3f}")
        
        report_lines.append('\\nEvaluation Complete')
        evaluation_report = '\\n'.join(report_lines)
        
        # Save outputs
        # Save metrics
        output_dir_metrics = os.path.dirname(eval_metrics_path)
        if output_dir_metrics and not os.path.exists(output_dir_metrics):
            os.makedirs(output_dir_metrics, exist_ok=True)
        
        with open(eval_metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # Save generated images
        output_dir_images = os.path.dirname(generated_images_path)
        if output_dir_images and not os.path.exists(output_dir_images):
            os.makedirs(output_dir_images, exist_ok=True)
        
        with open(generated_images_path, 'wb') as f:
            pickle.dump(generated_images, f)
        
        # Save evaluation report
        output_dir_report = os.path.dirname(evaluation_report_path)
        if output_dir_report and not os.path.exists(output_dir_report):
            os.makedirs(output_dir_report, exist_ok=True)
        
        with open(evaluation_report_path, 'w') as f:
            f.write(evaluation_report)
        
        print('Evaluation Complete')
        print('Saved metrics to: ' + eval_metrics_path)
        print('Saved ' + str(len(generated_images)) + ' images to: ' + generated_images_path)
        print('\\n' + evaluation_report)
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputPath: trained_model}
      - {inputPath: test_data}
      - {inputValue: config}
      - {outputPath: eval_metrics}
      - {outputPath: generated_images}
      - {outputPath: evaluation_report}
